{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3028ecd-a924-40cd-a9fb-919ab9b3f8b4",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".reveal pre code {\n",
       "    max-height: none;\n",
       "    font-size: 90%;\n",
       "}\n",
       ".rise-enabled .text_cell {\n",
       "    font-size: 150%;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "HTML{String}(\"<style>\\n.reveal pre code {\\n    max-height: none;\\n    font-size: 90%;\\n}\\n.rise-enabled .text_cell {\\n    font-size: 150%;\\n}\\n</style>\\n\")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENV[\"LINES\"] = 10;\n",
    "ENV[\"COLUMNS\"] = 80;\n",
    "\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".reveal pre code {\n",
    "    max-height: none;\n",
    "    font-size: 90%;\n",
    "}\n",
    ".rise-enabled .text_cell {\n",
    "    font-size: 150%;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63fb033",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Julia + Jupyter + GPU = ‚öóÔ∏èüî¨üß¨ü•∞\n",
    "\n",
    "Marius Millea (Project Scientist @ UC Davis in Cosmology)\n",
    "\n",
    "NERSC GPU Science Day, Oct 12, 2023\n",
    "\n",
    "Thanks to: Tim Besard + CUDA.jl/Julia contributors, Johannes Blaschke, Rollin Thomas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf0a160",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "I work on analyzing maps of the Cosmic Microwave Background. Using tiny distortions imprinted by gravitational lensing, we can make maps of where all the dark matter is in the universe. We do so by solving **millions-of-dimensional Bayesian inference problems.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b359fbe",
   "metadata": {},
   "source": [
    "<video controls autoplay loop muted width=\"1800\" height=\"600\" source src=\"kappa_forecast.mp4\" type=\"video/mp4\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df18b60c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Our basic code building blocks are array broadcasts and FFTs, which is perfectly suited for GPU. Our group has been using GPUs since the Cori GPU testbed days.\n",
    "\n",
    "But this talk is not about science, but instead **sharing the workflow we've developed over the last ~5 years.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a614681",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "* Julia + Jupyter + GPU motivation\n",
    "* Julia CUDA Installation\n",
    "* Basic and advanced Julia CUDA usage\n",
    "* Multi-GPU workflows for embarrasingly parallel problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b6d67e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdb55c5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Julia\n",
    "    * interactive but fast\n",
    "    * powerful and flexible\n",
    "    * less boilerplate: code looks like science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6228a33e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Jupyter\n",
    "    * convenient for interactive work\n",
    "    * fast iterative development workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97cfc77",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* GPU\n",
    "    * duh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea42a49-3f34-4061-83f3-d55b3f9edbdc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bfb764",
   "metadata": {},
   "source": [
    "Julia/CUDA install is drop-dead simple. Julia's CUDA package provides compatible binary drivers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1600af3",
   "metadata": {},
   "source": [
    "```shell\n",
    "$ curl -fsSL https://install.julialang.org | sh\n",
    "$ julia\n",
    "pkg> add CUDA # ~2min\n",
    "   Resolving package versions...\n",
    "   Installed CUDA_Driver_jll ‚îÄ‚îÄ v0.6.0+3\n",
    "   Installed LLVMExtra_jll ‚îÄ‚îÄ‚îÄ‚îÄ v0.0.26+0\n",
    "   ...\n",
    "   Installed CUDA ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ v5.0.0\n",
    " Downloading artifact: CUDA_Driver\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1318c9b9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "(Easy to select CUDA version _per project_ with e.g. `CUDA.set_runtime_version!(v\"11.4\")`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf466c7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "I recommend this fully native Julia install over using any `modules`, i.e. I don't even have the `gpu` module loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "349bac64-41a5-488a-888c-b9305be35e28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Currently Loaded Modules:\n",
      "  1) craype-x86-milan                        8) cray-mpich/8.1.25\n",
      "  2) libfabric/1.15.2.0                      9) craype/2.7.20\n",
      "  3) craype-network-ofi                     10) gcc/11.2.0\n",
      "  4) xpmem/2.6.2-2.5_2.27__gd067c3f.shasta  11) perftools-base/23.03.0\n",
      "  5) PrgEnv-gnu/8.3.3                       12) cpe/23.03\n",
      "  6) cray-dsmml/0.2.2                       13) xalt/2.10.2\n",
      "  7) cray-libsci/23.02.1.1                  14) cray-python/3.9.13.1   (dev)\n",
      "\n",
      "  Where:\n",
      "   dev:  Development Tools and Programming Languages\n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "; module list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdcec68",
   "metadata": {},
   "source": [
    "This has proven robust across many clusters I've tried."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d35ade1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Checking everything is installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02f49430-09f8-460a-a439-4cf0c3f93b8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e3bb8bd-d25f-4c8b-9b2d-5f13c681bc75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA runtime 12.2, artifact installation\n",
      "CUDA driver 12.2\n",
      "NVIDIA driver 525.105.17, originally for CUDA 12.0\n",
      "\n",
      "CUDA libraries: \n",
      "- CUBLAS: 12.2.5\n",
      "- CURAND: 10.3.3\n",
      "- CUFFT: 11.0.8\n",
      "- CUSOLVER: 11.5.2\n",
      "- CUSPARSE: 12.1.2\n",
      "- CUPTI: 20.0.0\n",
      "- NVML: 12.0.0+525.105.17\n",
      "\n",
      "Julia packages: \n",
      "- CUDA: 5.0.0\n",
      "- CUDA_Driver_jll: 0.6.0+3\n",
      "- CUDA_Runtime_jll: 0.9.2+0\n",
      "\n",
      "Toolchain:\n",
      "- Julia: 1.9.3\n",
      "- LLVM: 14.0.6\n",
      "- PTX ISA support: 3.2, 4.0, 4.1, 4.2, 4.3, 5.0, 6.0, 6.1, 6.3, 6.4, 6.5, 7.0, 7.1, 7.2, 7.3, 7.4, 7.5\n",
      "- Device capability support: sm_37, sm_50, sm_52, sm_53, sm_60, sm_61, sm_62, sm_70, sm_72, sm_75, sm_80, sm_86\n",
      "\n",
      "4 devices:\n",
      "  0: NVIDIA A100-SXM4-40GB (sm_80, 39.389 GiB / 40.000 GiB available)\n",
      "  1: NVIDIA A100-SXM4-40GB (sm_80, 39.389 GiB / 40.000 GiB available)\n",
      "  2: NVIDIA A100-SXM4-40GB (sm_80, 39.389 GiB / 40.000 GiB available)\n",
      "  3: NVIDIA A100-SXM4-40GB (sm_80, 39.389 GiB / 40.000 GiB available)\n"
     ]
    }
   ],
   "source": [
    "CUDA.versioninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5734821-7675-41c5-a547-a02157039f4c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c90f66cf-19ac-4d6d-b4b1-0e6e4e3febd9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000-element Vector{Float64}:\n",
       " 0.20079028039355207\n",
       " 0.2551683713911349\n",
       " 0.07850631788245288\n",
       " ‚ãÆ\n",
       " 0.18280216971091756\n",
       " 0.5304310135460691"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = rand(10_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e854be87-f7ec-4206-8410-698b1242cf33",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 0.20079029\n",
       " 0.25516838\n",
       " 0.07850632\n",
       " ‚ãÆ\n",
       " 0.18280217\n",
       " 0.53043103"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carr = cu(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "470104fb-7a46-4a34-b6e8-f32e9230b930",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 1.1994438\n",
       " 1.2524083\n",
       " 1.0784256\n",
       " ‚ãÆ\n",
       " 1.1817858\n",
       " 1.5059052"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sin.(carr) .+ 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009c3528",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lets benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4e7e49f-a8d9-4656-8e71-c03aa4f95fc0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "using BenchmarkTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d91b6b2-e853-4536-8109-1eec8e4ab8da",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  83.051 Œºs (41 allocations: 2.00 KiB)\n"
     ]
    }
   ],
   "source": [
    "@btime CUDA.@sync sin.(carr) .+ 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "321dcf26-1d65-49c5-8c25-79df78c8d97d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  68.599 ms (6 allocations: 76.29 MiB)\n"
     ]
    }
   ],
   "source": [
    "@btime sin.(arr) .+ 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "970e23ed-a0ef-401e-a22f-df98921f7ba8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiler ran for 326.63 ¬µs, capturing 11 events.\n",
      "\n",
      "Host-side activity: calling CUDA APIs took 61.75 ¬µs (18.91% of the trace)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ\u001b[1m Time (%) \u001b[0m‚îÇ\u001b[1m     Time \u001b[0m‚îÇ\u001b[1m Calls \u001b[0m‚îÇ\u001b[1m Avg time \u001b[0m‚îÇ\u001b[1m Min time \u001b[0m‚îÇ\u001b[1m Max time \u001b[0m‚îÇ\u001b[1m Name                    \u001b[0m‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ    9.56% ‚îÇ\u001b[31m 31.23 ¬µs \u001b[0m‚îÇ     1 ‚îÇ 31.23 ¬µs ‚îÇ 31.23 ¬µs ‚îÇ 31.23 ¬µs ‚îÇ\u001b[1m cuLaunchKernel          \u001b[0m‚îÇ\n",
      "‚îÇ    7.66% ‚îÇ 25.03 ¬µs ‚îÇ     1 ‚îÇ 25.03 ¬µs ‚îÇ 25.03 ¬µs ‚îÇ 25.03 ¬µs ‚îÇ cuMemAllocFromPoolAsync ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "Device-side activity: GPU was busy for 81.54 ¬µs (24.96% of the trace)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ\u001b[1m Time (%) \u001b[0m‚îÇ\u001b[1m     Time \u001b[0m‚îÇ\u001b[1m Calls \u001b[0m‚îÇ\u001b[1m Avg time \u001b[0m‚îÇ\u001b[1m Min time \u001b[0m‚îÇ\u001b[1m Max time \u001b[0m‚îÇ\u001b[1m Name                                                                                                        \u001b[0m ‚ãØ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ   24.96% ‚îÇ\u001b[31m 81.54 ¬µs \u001b[0m‚îÇ     1 ‚îÇ 81.54 ¬µs ‚îÇ 81.54 ¬µs ‚îÇ 81.54 ¬µs ‚îÇ\u001b[1m _Z16broadcast_kernel15CuKernelContext13CuDeviceArrayI7Float32Li1ELi1EE11BroadcastedI12CuArrayStyleILi1EE5Tup\u001b[0m ‚ãØ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\u001b[36m                                                                                                                                                               1 column omitted\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "CUDA.@profile sin.(carr) .+ 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4506ce3c-0868-4eea-bbc7-b3004d589e60",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Power of Julia (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fadaaea",
   "metadata": {},
   "source": [
    "In Julia, you can easily put many arbitrary objects on GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "382f1b13-28ee-4a9d-b38a-bec483b0254e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "struct Point{T}\n",
    "    x :: T\n",
    "    y :: T\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35ae9488-9d0b-4c43-8012-5b476c2bdfed",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100-element CuArray{Point{Float64}, 1, CUDA.Mem.DeviceBuffer}:\n",
       " Point{Float64}(0.8490008946627912, 0.48658520886875856)\n",
       " Point{Float64}(0.06977616429006461, 0.2501647436222665)\n",
       " Point{Float64}(0.6924522464442648, 0.2656146874146924)\n",
       " ‚ãÆ\n",
       " Point{Float64}(0.8748131265382463, 0.2480993353552592)\n",
       " Point{Float64}(0.49503190701987954, 0.13355513219798332)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = Point.(rand(100), rand(100))\n",
    "carr = cu(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9555b43",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In e.g. Jax/PyTorch/TF, the only things you can stick inside of CUDA arrays are Int/Float/Complex. In Julia, anything with a static memory layout is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "528d1f95-c491-4e96-94cb-e0c976c511d6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "distance_from_origin (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_from_origin(p::Point) = sqrt(p.x^2 + p.y^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba152df7-0d00-477c-9eb1-ddfe355b4cfe",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100-element CuArray{Float64, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 0.9785538741572041\n",
       " 0.2597135191988057\n",
       " 0.7416476763100615\n",
       " ‚ãÆ\n",
       " 0.9093136348737674\n",
       " 0.5127314719267381"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_from_origin.(carr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbac985e-bc4e-4579-a709-ab0b37399fea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "832d529f-7dfd-48f4-a5ae-eb80b08c3fa9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "distance_from_origin_bad (generic function with 1 method)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function distance_from_origin_bad(p::Point)\n",
    "    sqrt(sum([p.x^2, p.y^2]))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "727c6377-d9d2-42e2-a8b4-45cef1ed4db8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "InvalidIRError: compiling MethodInstance for (::GPUArrays.var\"#broadcast_kernel#32\")(::CUDA.CuKernelContext, ::CuDeviceVector{Float64, 1}, ::Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{1}, Tuple{Base.OneTo{Int64}}, typeof(distance_from_origin_bad), Tuple{Base.Broadcast.Extruded{CuDeviceVector{Point{Float64}, 1}, Tuple{Bool}, Tuple{Int64}}}}, ::Int64) resulted in invalid LLVM IR\n\u001b[31mReason: unsupported call through a literal pointer\u001b[39m\u001b[31m (call to ijl_alloc_array_1d)\u001b[39m\nStacktrace:\n  [1] \u001b[0m\u001b[1mArray\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mboot.jl:477\u001b[24m\u001b[39m\n  [2] \u001b[0m\u001b[1mArray\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mboot.jl:486\u001b[24m\u001b[39m\n  [3] \u001b[0m\u001b[1msimilar\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mabstractarray.jl:884\u001b[24m\u001b[39m\n  [4] \u001b[0m\u001b[1msimilar\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mabstractarray.jl:883\u001b[24m\u001b[39m\n  [5] \u001b[0m\u001b[1m_array_for\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4marray.jl:671\u001b[24m\u001b[39m\n  [6] \u001b[0m\u001b[1m_array_for\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4marray.jl:674\u001b[24m\u001b[39m\n  [7] \u001b[0m\u001b[1mvect\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4marray.jl:126\u001b[24m\u001b[39m\n  [8] \u001b[0m\u001b[1mdistance_from_origin_bad\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mIn[19]:2\u001b[24m\u001b[39m\n  [9] \u001b[0m\u001b[1m_broadcast_getindex_evalf\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mbroadcast.jl:683\u001b[24m\u001b[39m\n [10] \u001b[0m\u001b[1m_broadcast_getindex\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mbroadcast.jl:656\u001b[24m\u001b[39m\n [11] \u001b[0m\u001b[1mgetindex\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mbroadcast.jl:610\u001b[24m\u001b[39m\n [12] \u001b[0m\u001b[1mbroadcast_kernel\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m~/.julia/packages/GPUArrays/EZkix/src/host/\u001b[39m\u001b[90m\u001b[4mbroadcast.jl:64\u001b[24m\u001b[39m\n\u001b[36m\u001b[1mHint\u001b[22m\u001b[39m\u001b[36m: catch this exception as `err` and call `code_typed(err; interactive = true)` to introspect the erronous code with Cthulhu.jl\u001b[39m",
     "output_type": "error",
     "traceback": [
      "InvalidIRError: compiling MethodInstance for (::GPUArrays.var\"#broadcast_kernel#32\")(::CUDA.CuKernelContext, ::CuDeviceVector{Float64, 1}, ::Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{1}, Tuple{Base.OneTo{Int64}}, typeof(distance_from_origin_bad), Tuple{Base.Broadcast.Extruded{CuDeviceVector{Point{Float64}, 1}, Tuple{Bool}, Tuple{Int64}}}}, ::Int64) resulted in invalid LLVM IR\n\u001b[31mReason: unsupported call through a literal pointer\u001b[39m\u001b[31m (call to ijl_alloc_array_1d)\u001b[39m\nStacktrace:\n  [1] \u001b[0m\u001b[1mArray\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mboot.jl:477\u001b[24m\u001b[39m\n  [2] \u001b[0m\u001b[1mArray\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mboot.jl:486\u001b[24m\u001b[39m\n  [3] \u001b[0m\u001b[1msimilar\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mabstractarray.jl:884\u001b[24m\u001b[39m\n  [4] \u001b[0m\u001b[1msimilar\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mabstractarray.jl:883\u001b[24m\u001b[39m\n  [5] \u001b[0m\u001b[1m_array_for\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4marray.jl:671\u001b[24m\u001b[39m\n  [6] \u001b[0m\u001b[1m_array_for\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4marray.jl:674\u001b[24m\u001b[39m\n  [7] \u001b[0m\u001b[1mvect\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4marray.jl:126\u001b[24m\u001b[39m\n  [8] \u001b[0m\u001b[1mdistance_from_origin_bad\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mIn[19]:2\u001b[24m\u001b[39m\n  [9] \u001b[0m\u001b[1m_broadcast_getindex_evalf\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mbroadcast.jl:683\u001b[24m\u001b[39m\n [10] \u001b[0m\u001b[1m_broadcast_getindex\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mbroadcast.jl:656\u001b[24m\u001b[39m\n [11] \u001b[0m\u001b[1mgetindex\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mbroadcast.jl:610\u001b[24m\u001b[39m\n [12] \u001b[0m\u001b[1mbroadcast_kernel\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m~/.julia/packages/GPUArrays/EZkix/src/host/\u001b[39m\u001b[90m\u001b[4mbroadcast.jl:64\u001b[24m\u001b[39m\n\u001b[36m\u001b[1mHint\u001b[22m\u001b[39m\u001b[36m: catch this exception as `err` and call `code_typed(err; interactive = true)` to introspect the erronous code with Cthulhu.jl\u001b[39m",
      "",
      "Stacktrace:",
      "  [1] check_ir(job::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams}, args::LLVM.Module)",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/2mJjc/src/validation.jl:147",
      "  [2] macro expansion",
      "    @ ~/.julia/packages/GPUCompiler/2mJjc/src/driver.jl:440 [inlined]",
      "  [3] macro expansion",
      "    @ ~/.julia/packages/TimerOutputs/RsWnF/src/TimerOutput.jl:253 [inlined]",
      "  [4] macro expansion",
      "    @ ~/.julia/packages/GPUCompiler/2mJjc/src/driver.jl:439 [inlined]",
      "  [5] emit_llvm(job::GPUCompiler.CompilerJob; libraries::Bool, toplevel::Bool, optimize::Bool, cleanup::Bool, only_entry::Bool, validate::Bool)",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/2mJjc/src/utils.jl:92",
      "  [6] codegen(output::Symbol, job::GPUCompiler.CompilerJob; libraries::Bool, toplevel::Bool, optimize::Bool, cleanup::Bool, strip::Bool, validate::Bool, only_entry::Bool, parent_job::Nothing)",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/2mJjc/src/driver.jl:129",
      "  [7] codegen",
      "    @ ~/.julia/packages/GPUCompiler/2mJjc/src/driver.jl:110 [inlined]",
      "  [8] compile(target::Symbol, job::GPUCompiler.CompilerJob; libraries::Bool, toplevel::Bool, optimize::Bool, cleanup::Bool, strip::Bool, validate::Bool, only_entry::Bool)",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/2mJjc/src/driver.jl:106",
      "  [9] compile",
      "    @ ~/.julia/packages/GPUCompiler/2mJjc/src/driver.jl:98 [inlined]",
      " [10] #1042",
      "    @ ~/.julia/packages/CUDA/nbRJk/src/compiler/compilation.jl:166 [inlined]",
      " [11] JuliaContext(f::CUDA.var\"#1042#1045\"{GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams}})",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/2mJjc/src/driver.jl:47",
      " [12] compile(job::GPUCompiler.CompilerJob)",
      "    @ CUDA ~/.julia/packages/CUDA/nbRJk/src/compiler/compilation.jl:165",
      " [13] actual_compilation(cache::Dict{Any, CuFunction}, src::Core.MethodInstance, world::UInt64, cfg::GPUCompiler.CompilerConfig{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams}, compiler::typeof(CUDA.compile), linker::typeof(CUDA.link))",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/2mJjc/src/execution.jl:125",
      " [14] cached_compilation(cache::Dict{Any, CuFunction}, src::Core.MethodInstance, cfg::GPUCompiler.CompilerConfig{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams}, compiler::Function, linker::Function)",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/2mJjc/src/execution.jl:103",
      " [15] macro expansion",
      "    @ ~/.julia/packages/CUDA/nbRJk/src/compiler/execution.jl:323 [inlined]",
      " [16] macro expansion",
      "    @ ./lock.jl:267 [inlined]",
      " [17] cufunction(f::GPUArrays.var\"#broadcast_kernel#32\", tt::Type{Tuple{CUDA.CuKernelContext, CuDeviceVector{Float64, 1}, Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{1}, Tuple{Base.OneTo{Int64}}, typeof(distance_from_origin_bad), Tuple{Base.Broadcast.Extruded{CuDeviceVector{Point{Float64}, 1}, Tuple{Bool}, Tuple{Int64}}}}, Int64}}; kwargs::Base.Pairs{Symbol, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})",
      "    @ CUDA ~/.julia/packages/CUDA/nbRJk/src/compiler/execution.jl:318",
      " [18] cufunction",
      "    @ ~/.julia/packages/CUDA/nbRJk/src/compiler/execution.jl:315 [inlined]",
      " [19] macro expansion",
      "    @ ~/.julia/packages/CUDA/nbRJk/src/compiler/execution.jl:104 [inlined]",
      " [20] #launch_heuristic#1087",
      "    @ ~/.julia/packages/CUDA/nbRJk/src/gpuarrays.jl:17 [inlined]",
      " [21] launch_heuristic",
      "    @ ~/.julia/packages/CUDA/nbRJk/src/gpuarrays.jl:15 [inlined]",
      " [22] _copyto!",
      "    @ ~/.julia/packages/GPUArrays/EZkix/src/host/broadcast.jl:70 [inlined]",
      " [23] copyto!",
      "    @ ~/.julia/packages/GPUArrays/EZkix/src/host/broadcast.jl:51 [inlined]",
      " [24] copy",
      "    @ ~/.julia/packages/GPUArrays/EZkix/src/host/broadcast.jl:42 [inlined]",
      " [25] materialize(bc::Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{1}, Nothing, typeof(distance_from_origin_bad), Tuple{CuArray{Point{Float64}, 1, CUDA.Mem.DeviceBuffer}}})",
      "    @ Base.Broadcast ./broadcast.jl:873",
      " [26] top-level scope",
      "    @ In[20]:1"
     ]
    }
   ],
   "source": [
    "distance_from_origin_bad.(carr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807107d4-73c8-473a-9d13-737c2311897c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Limitations on code in functions that will be compiled for GPU:\n",
    "* No calls to CPU functions\n",
    "   * E.g. creating Arrays (use StaticArrays.jl instead)\n",
    "* No _dynamic dispatch_\n",
    "   * Code should be _type stable_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cae2fe-6444-479e-a1f6-98acb5800ba5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Power of Julia (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf0865",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You can also directly write kernels in Julia, giving the full power and flexibility of CUDA kernel programming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1ad299e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "my_kernel (generic function with 1 method)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function my_kernel(carr_out, carr)   \n",
    "    start = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n",
    "    stride = blockDim().x * gridDim().x\n",
    "    len = length(carr)\n",
    "    for i = start:stride:len  # \"grid-stride\" loop\n",
    "        carr_out[i] = sin(carr[i]) + 1\n",
    "    end\n",
    "    return\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "635c6648",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "carr = cu(rand(10_000_000))\n",
    "carr_out = similar(carr);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ab21fce",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CUDA.HostKernel for my_kernel(CuDeviceVector{Float32, 1}, CuDeviceVector{Float32, 1})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@cuda threads=256 my_kernel(carr_out, carr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce0793f0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 1.264759\n",
       " 1.3415114\n",
       " 1.0137947\n",
       " ‚ãÆ\n",
       " 1.7018087\n",
       " 1.215888"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carr_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0c5d3c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "See [Kernel Programming](https://cuda.juliagpu.org/stable/api/kernel/) for full list of CUDA.jl kernel programming capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ae7b4e-0547-4f11-83aa-94433a2af38d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-GPU (single node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2ead7fe-334b-4c1a-a6d6-909dd01d7ff5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CUDA.DeviceIterator() for 4 devices:\n",
       "0. NVIDIA A100-SXM4-40GB\n",
       "1. NVIDIA A100-SXM4-40GB\n",
       "2. NVIDIA A100-SXM4-40GB\n",
       "3. NVIDIA A100-SXM4-40GB"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUDA.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc5b554c-33e9-4512-b0e3-41014eb3106b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CuDevice(0): NVIDIA A100-SXM4-40GB"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUDA.device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e3976cb-3305-44ac-97b7-3dd7363646e7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CuDevice(1): NVIDIA A100-SXM4-40GB"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUDA.device!(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65eaa3e0-3097-44df-8402-c43a20c88d8e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  85.705 Œºs (41 allocations: 2.00 KiB)\n"
     ]
    }
   ],
   "source": [
    "arr = rand(10_000_000)\n",
    "carr = cu(arr)\n",
    "@btime CUDA.@sync sin.(carr) .+ 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbee4a7-f1fc-43c3-a2ac-95e40758bc07",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "CUDA.jl does its own memory management, so before switching back to GPU 0, give back memory (don't usually have to think about this unless you use the same GPU from multiple processes, which for the purpose of this demo I do):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0da831ed-5f8e-4886-b7d6-1fbc7acff2eb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "GC.gc()\n",
    "CUDA.reclaim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a29fbf8c-f564-4d65-a4d9-b73321fa11c7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CuDevice(0): NVIDIA A100-SXM4-40GB"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUDA.device!(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e376af19",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can use multiple GPUs via Julia processes, tasks, or threads. \n",
    "\n",
    "The most robust and easy way I have found (as of 2023), which I recommend starting with, is per-_process_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2f51e76-6517-4a49-95e2-5a3459d63a81",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "using Distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d940d90-6514-4d8d-9e3b-07da5a60d32f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Int64}:\n",
       " 2\n",
       " 3\n",
       " 4"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addprocs(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "49108a26-d4fb-4fa4-905c-a71a71d8f5a2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@everywhere using CUDA, BenchmarkTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4094404e-7f67-488c-ba27-71270b8284b2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, CuDevice(0))\n",
      "      From worker 3:\t(3, CuDevice(0))\n",
      "      From worker 2:\t(2, CuDevice(0))\n",
      "      From worker 4:\t(4, CuDevice(0))\n"
     ]
    }
   ],
   "source": [
    "@everywhere procs() println((myid(), CUDA.device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9202be9-166c-4416-abf6-57b9bdd9da47",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@everywhere procs() CUDA.device!(myid()-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "77aa9ba8-faaf-42bd-8c58-86c4423a468b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, CuDevice(0))\n",
      "      From worker 2:\t(2, CuDevice(1))\n",
      "      From worker 3:\t(3, CuDevice(2))\n",
      "      From worker 4:\t(4, CuDevice(3))\n"
     ]
    }
   ],
   "source": [
    "@everywhere procs() println((myid(), CUDA.device()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119909c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lets run our benchmark in parallel across all GPUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7edbb80-4e02-462c-b2ca-127c35b14130",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  85.255 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 3:\t  81.938 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 2:\t  82.238 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 4:\t  81.597 Œºs (37 allocations: 1.91 KiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4-element Vector{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}:\n",
       " Float32[1.8228395, 1.0992318, 1.759596, 1.8266902, 1.1692553, 1.3392247, 1.7539189, 1.7294312, 1.705107, 1.6065087  ‚Ä¶  1.4714574, 1.0195016, 1.0635127, 1.5888524, 1.7992377, 1.333357, 1.2087038, 1.0803039, 1.7202525, 1.5920569]\n",
       " Float32[1.8228395, 1.0992318, 1.759596, 1.8266902, 1.1692553, 1.3392247, 1.7539189, 1.7294312, 1.705107, 1.6065087  ‚Ä¶  1.4714574, 1.0195016, 1.0635127, 1.5888524, 1.7992377, 1.333357, 1.2087038, 1.0803039, 1.7202525, 1.5920569]\n",
       " Float32[1.8228395, 1.0992318, 1.759596, 1.8266902, 1.1692553, 1.3392247, 1.7539189, 1.7294312, 1.705107, 1.6065087  ‚Ä¶  1.4714574, 1.0195016, 1.0635127, 1.5888524, 1.7992377, 1.333357, 1.2087038, 1.0803039, 1.7202525, 1.5920569]\n",
       " Float32[1.8228395, 1.0992318, 1.759596, 1.8266902, 1.1692553, 1.3392247, 1.7539189, 1.7294312, 1.705107, 1.6065087  ‚Ä¶  1.4714574, 1.0195016, 1.0635127, 1.5888524, 1.7992377, 1.333357, 1.2087038, 1.0803039, 1.7202525, 1.5920569]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let\n",
    "    carr = cu(rand(10_000_000))\n",
    "    pmap(WorkerPool(procs()), 1:4) do i\n",
    "        @btime CUDA.@sync sin.($carr) .+ 1\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7edc333-7c3a-4760-95e0-b7e10f2789c8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note, `carr` was defined and moved to GPU on the master process. Julia automatically sent it to the worker GPUs, then automatically sent the results back to the master GPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1c3999-845c-4b3e-8e54-a2215aa3801b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In doing so, the array passed through CPU memory, so its not the most efficient (but its the easiest)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fafedca-7e29-4c4d-86a8-9edcca2e4718",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To go straight GPU-to-GPU, you can use _unified memory_ on a single-node, or CUDA MPI transport (later this talk)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bfa320-711a-487f-92ef-526728c517c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-GPU (multiple nodes, elastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cbe338f9-afc9-4648-9b27-f59c1c21e543",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "using ClusterManagers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d8bf1b4a-344f-4d43-b485-ae04335339cc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "em = ElasticManager(\n",
    "    # Perlmutter specific ‚Üì\n",
    "    addr = IPv4(first(filter(!isnothing, match.(r\"inet (.*)/.*hsn0\", readlines(`ip a show`)))).captures[1]),\n",
    "    port = 0\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4f2284ef-47ab-40fa-8e11-28f2c4bdefba",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticManager:\n",
       "  Active workers : [ 5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36]\n",
       "  Number of workers to be added  : 0\n",
       "  Terminated workers : []\n",
       "  Worker connect command : \n",
       "    /global/u1/m/marius/.julia/juliaup/julia-1.9.3+0.x64.linux.gnu/bin/julia --project=/global/u1/m/marius/work/gpu_science_day_julia/Project.toml -e 'using ClusterManagers; ClusterManagers.elastic_worker(\"6Ty6RCu5sIy5CedV\",\"10.249.6.77\",35449)'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44088681-107f-4268-8b5b-d1e7c173fd98",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now submit a job, e.g. with:\n",
    "```bash\n",
    "salloc -C gpu -q regular -t 00:30:00 --cpus-per-task 32  --gpus-per-task 1 --ntasks-per-node 4 --nodes 8 -A mp107\n",
    "```\n",
    "then run the \"worker connect command\" printed above (could also do all-in-one as a batch job)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ac85b2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "With more GPUs across different nodes, its more complex to assign one unique GPU to each process. Instead we can use this utility function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "582fa7c0-1f84-46b7-852e-5afc9df059ee",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "using CUDADistributedTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a6d35638-7a08-4b9f-88ae-787a54b4b781",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m‚îå \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mProcesses (36):\n",
      "\u001b[36m\u001b[1m‚îÇ \u001b[22m\u001b[39m (myid = 1, host = nid001293, device = CuDevice(0): NVIDIA A100-SXM4-40GB 1c40175b))\n",
      "\u001b[36m\u001b[1m‚îÇ \u001b[22m\u001b[39m (myid = 2, host = nid001293, device = CuDevice(1): NVIDIA A100-SXM4-40GB f179efe2))\n",
      "\u001b[36m\u001b[1m‚îÇ \u001b[22m\u001b[39m (myid = 3, host = nid001293, device = CuDevice(2): NVIDIA A100-SXM4-40GB 36d32866))\n",
      "\u001b[36m\u001b[1m‚îÇ \u001b[22m\u001b[39m (myid = 4, host = nid001293, device = CuDevice(3): NVIDIA A100-SXM4-40GB 634451b9))\n",
      "\u001b[36m\u001b[1m‚îÇ \u001b[22m\u001b[39m (myid = 5, host = nid002532, device = CuDevice(0): NVIDIA A100-SXM4-40GB 892d65ed))\n",
      "\u001b[36m\u001b[1m‚îÇ \u001b[22m\u001b[39m (myid = 6, host = nid002532, device = CuDevice(0): NVIDIA A100-SXM4-40GB 0212ac25))\n",
      "\u001b[36m\u001b[1m‚îÇ \u001b[22m\u001b[39m (myid = 7, host = nid002532, device = CuDevice(0): NVIDIA A100-SXM4-40GB 9f1b5f73))\n",
      "\u001b[36m\u001b[1m‚îÇ \u001b[22m\u001b[39m (myid = 8, host = nid002532, device = CuDevice(0): NVIDIA A100-SXM4-40GB b9ac9c36))\n",
      "\u001b[36m\u001b[1m‚îÇ \u001b[22m\u001b[39m (myid = 9, host = nid002536, device = CuDevice(0): NVIDIA A100-SXM4-40GB 1ffb4f18))\n",
      "\u001b[36m\u001b[1m‚îÇ \u001b[22m\u001b[39m (myid = 10, host = nid002536, device = CuDevice(0): NVIDIA A100-SXM4-40GB a25217d5))\n",
      "\u001b[36m\u001b[1m‚îÇ \u001b[22m\u001b[39m (myid = 11, host = nid002536, device = CuDevice(0): NVIDIA A100-SXM4-40GB 2ad12529))\n",
      "\u001b[36m\u001b[1m‚îÇ \u001b[22m\u001b[39m (myid = 12, host = nid002536, device = CuDevice(0): NVIDIA A100-SXM4-40GB 91817c8d))\n",
      "\u001b[36m\u001b[1m‚îÇ \u001b[22m\u001b[39m (myid = 13, host = nid003320, device = CuDevice(0): NVIDIA A100-SXM4-40GB 6f8ab1df))\n",
      "\u001b[36m\u001b[1m‚îÇ \u001b[22m\u001b[39m (myid = 14, host = nid003320, device = CuDevice(0): NVIDIA A100-SXM4-40GB 014e077e))\n",
      "\u001b[36m\u001b[1m‚îÇ \u001b[22m\u001b[39m (myid = 15, host = nid003320, device = CuDevice(0): NVIDIA A100-SXM4-40GB 38a58e41))\n",
      "\u001b[36m\u001b[1m‚îÇ \u001b[22m\u001b[39m (myid = 16, host = nid003320, device = CuDevice(0): NVIDIA A100-SXM4-40GB a860a000))\n",
      "\u001b[36m\u001b[1m‚îÇ \u001b[22m\u001b[39m (myid = 17, host = nid003316, device = CuDevice(0): NVIDIA A100-SXM4-40GB fdfe719c))\n",
      "\u001b[36m\u001b[1m‚îÇ \u001b[22m\u001b[39m (myid = 18, host = nid003316, device = CuDevice(0): NVIDIA A100-SXM4-40GB 547b9f5c))\n",
      "\u001b[36m\u001b[1m‚îÇ \u001b[22m\u001b[39m (myid = 19, host = nid003316, device = CuDevice(0): NVIDIA A100-SXM4-40GB ee15a3a3))\n",
      "\u001b[36m\u001b[1m‚îÇ \u001b[22m\u001b[39m (myid = 20, host = nid003316, device = CuDevice(0): NVIDIA A100-SXM4-40GB 32342d63))\n",
      "\u001b[36m\u001b[1m‚îÇ \u001b[22m\u001b[39m (myid = 21, host = nid002533, device = CuDevice(0): NVIDIA A100-SXM4-40GB f5695274))\n",
      "\u001b[36m\u001b[1m‚îÇ \u001b[22m\u001b[39m (myid = 22, host = nid002533, device = CuDevice(0): NVIDIA A100-SXM4-40GB 4cdbb673))\n",
      "\u001b[36m\u001b[1m‚îÇ \u001b[22m\u001b[39m (myid = 23, host = nid002533, device = CuDevice(0): NVIDIA A100-SXM4-40GB 0130c469))\n",
      "\u001b[36m\u001b[1m‚îÇ \u001b[22m\u001b[39m (myid = 24, host = nid003317, device = CuDevice(0): NVIDIA A100-SXM4-40GB 5aeab6da))\n",
      "\u001b[36m\u001b[1m‚îÇ \u001b[22m\u001b[39m (myid = 25, host = nid003317, device = CuDevice(0): NVIDIA A100-SXM4-40GB 44081cfa))\n",
      "\u001b[36m\u001b[1m‚îÇ \u001b[22m\u001b[39m (myid = 26, host = nid003317, device = CuDevice(0): NVIDIA A100-SXM4-40GB 0a4aa27d))\n",
      "\u001b[36m\u001b[1m‚îÇ \u001b[22m\u001b[39m (myid = 27, host = nid003317, device = CuDevice(0): NVIDIA A100-SXM4-40GB 06c81dc5))\n",
      "\u001b[36m\u001b[1m‚îÇ \u001b[22m\u001b[39m (myid = 28, host = nid002533, device = CuDevice(0): NVIDIA A100-SXM4-40GB 20734e54))\n",
      "\u001b[36m\u001b[1m‚îÇ \u001b[22m\u001b[39m (myid = 29, host = nid003313, device = CuDevice(0): NVIDIA A100-SXM4-40GB 60aa76e5))\n",
      "\u001b[36m\u001b[1m‚îÇ \u001b[22m\u001b[39m (myid = 30, host = nid003313, device = CuDevice(0): NVIDIA A100-SXM4-40GB 249cdaab))\n",
      "\u001b[36m\u001b[1m‚îÇ \u001b[22m\u001b[39m (myid = 31, host = nid003313, device = CuDevice(0): NVIDIA A100-SXM4-40GB da05c388))\n",
      "\u001b[36m\u001b[1m‚îÇ \u001b[22m\u001b[39m (myid = 32, host = nid003313, device = CuDevice(0): NVIDIA A100-SXM4-40GB dc1c5e9d))\n",
      "\u001b[36m\u001b[1m‚îÇ \u001b[22m\u001b[39m (myid = 33, host = nid003321, device = CuDevice(0): NVIDIA A100-SXM4-40GB 3b627630))\n",
      "\u001b[36m\u001b[1m‚îÇ \u001b[22m\u001b[39m (myid = 34, host = nid003321, device = CuDevice(0): NVIDIA A100-SXM4-40GB 76cf68e2))\n",
      "\u001b[36m\u001b[1m‚îÇ \u001b[22m\u001b[39m (myid = 35, host = nid003321, device = CuDevice(0): NVIDIA A100-SXM4-40GB b2cb91b4))\n",
      "\u001b[36m\u001b[1m‚îî \u001b[22m\u001b[39m (myid = 36, host = nid003321, device = CuDevice(0): NVIDIA A100-SXM4-40GB 4f421754))\n"
     ]
    }
   ],
   "source": [
    "CUDADistributedTools.assign_GPU_workers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce91d09",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's run parallel benchmarks again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6c79cdcd-2c28-4c03-aa33-9aac7c239fbe",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@everywhere using CUDA, BenchmarkTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "199904a6-ccb2-4bcb-b358-b9a9830fdc6a",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  85.174 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 3:\t  81.828 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 4:\t  81.768 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 2:\t  82.759 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 5:\t  81.707 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 8:\t  81.758 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 7:\t  81.897 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 9:\t  82.840 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 10:\t  81.388 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 12:\t  81.898 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 11:\t  81.638 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 6:\t  83.030 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 15:\t  81.608 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 13:\t  83.251 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 16:\t  82.169 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 14:\t  83.382 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 19:\t  82.439 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 18:\t  82.630 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 22:\t  81.197 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 26:\t  82.680 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 17:\t  83.001 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 21:\t  81.497 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 29:\t  81.617 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 20:\t  82.448 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 24:\t  81.669 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 25:\t  82.230 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 33:\t  81.507 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 32:\t  81.618 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 30:\t  81.447 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 35:\t  81.908 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 31:\t  81.718 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 27:\t  82.640 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 34:\t  81.577 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 28:\t  82.680 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 23:\t  81.618 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 36:\t  81.768 Œºs (37 allocations: 1.91 KiB)\n"
     ]
    }
   ],
   "source": [
    "let\n",
    "    carr = cu(rand(10_000_000))\n",
    "    pmap(WorkerPool(procs()), 1:nprocs()) do i\n",
    "        @btime CUDA.@sync sin.($carr) .+ 1\n",
    "        return nothing\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e7a87f-4286-4a7e-a087-304ed97b32da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-GPU (multiple nodes, MPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cbf52d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Installing MPI for Julia and configuring:\n",
    "```julia\n",
    "pkg> add MPI MPIPreferences\n",
    "\n",
    "julia> MPIPreferences.use_system_binary(;vendor=\"cray\", mpiexec=\"srun\") # <- options are Perlmutter specific\n",
    "\n",
    "‚îå Info: MPI implementation identified\n",
    "‚îÇ   libmpi = \"libmpi_gnu_91.so\"\n",
    "‚îÇ   version_string = \"MPI VERSION    : CRAY MPICH version 8.1.25.17 (ANL base 3.4a2)\\nMPI BUILD INFO : Sun Feb 26 15:15 2023 (git hash aecd99f)\\n\"\n",
    "‚îÇ   impl = \"CrayMPICH\"\n",
    "‚îÇ   version = v\"8.1.25\"\n",
    "‚îî   abi = \"MPICH\"\n",
    "‚îå Info: MPIPreferences changed\n",
    "‚îÇ   binary = \"system\"\n",
    "‚îÇ   libmpi = \"libmpi_gnu_91.so\"\n",
    "‚îÇ   abi = \"MPICH\"\n",
    "‚îÇ   mpiexec = \"srun\"\n",
    "‚îÇ   preloads =\n",
    "‚îÇ    1-element Vector{String}:\n",
    "‚îÇ     \"libmpi_gtl_cuda.so\"\n",
    "‚îî   preloads_env_switch = \"MPICH_GPU_SUPPORT_ENABLED\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5520ab5e-f687-41a3-bc12-2293241040c2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "(This works thanks to among others NERSC's Johannes Blaschke's contributions to MPI.jl) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f5780f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "You can put SLURM script and Julia script in one file \n",
    "`test_script.jl`:\n",
    "\n",
    "```julia\n",
    "#!/bin/bash\n",
    "#SBATCH -C gpu -q regular -A mp107\n",
    "#SBATCH -t 00:05:00 \n",
    "#SBATCH --cpus-per-task 32 --gpus-per-task 1 --ntasks-per-node 4 --nodes 4\n",
    "#=\n",
    "srun /global/u1/m/marius/.julia/juliaup/julia-1.9.3+0.x64.linux.gnu/bin/julia $(scontrol show job $SLURM_JOBID | awk -F= '/Command=/{print $2}')\n",
    "exit 0\n",
    "# =#\n",
    "\n",
    "using MPIClusterManagers, Distributed, CUDA, BenchmarkTools\n",
    "mgr = MPIClusterManagers.start_main_loop(MPIClusterManagers.MPI_TRANSPORT_ALL)\n",
    "\n",
    "let\n",
    "    carr = cu(rand(10_000_000))\n",
    "    pmap(WorkerPool(procs()), 1:nprocs()) do i\n",
    "        @btime CUDA.@sync sin.($carr) .+ 1\n",
    "    end\n",
    "end\n",
    "\n",
    "MPIClusterManagers.stop_main_loop(mgr)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd5c933",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Then `sbatch test_script.jl`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9775252-0ffd-47b9-9d3b-f161257bd22b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Here, movement of memory between GPUs will happen via CUDA MPI transport üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da0ab8d-683f-4af1-8df5-2950d74550af",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-GPU (multiple nodes, MPI, notebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1bb015-b678-49e3-82f8-ec8a86e42c07",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Some code in a notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7e99065e-7209-4338-8f38-a1a389b5e238",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  85.515 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 5:\t  81.698 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 6:\t  81.728 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 2:\t  81.918 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 8:\t  81.697 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 7:\t  81.637 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 9:\t  81.338 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 10:\t  81.197 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 3:\t  81.867 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 13:\t  81.968 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 4:\t  81.838 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 11:\t  81.668 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 12:\t  82.049 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 15:\t  80.727 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 14:\t  81.378 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 16:\t  82.159 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 17:\t  81.297 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 18:\t  81.277 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 20:\t  81.357 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 21:\t  81.899 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 19:\t  81.637 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 23:\t  81.597 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 22:\t  81.558 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 25:\t  81.738 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 24:\t  81.587 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 26:\t  81.558 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 28:\t  81.688 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 27:\t  81.808 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 29:\t  81.798 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 30:\t  82.329 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 31:\t  81.658 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 33:\t  81.668 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 32:\t  81.788 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 35:\t  81.457 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 34:\t  81.778 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 36:\t  81.838 Œºs (37 allocations: 1.91 KiB)\n"
     ]
    }
   ],
   "source": [
    "let\n",
    "    carr = cu(rand(10_000_000))\n",
    "    pmap(WorkerPool(procs()), 1:nprocs()) do i\n",
    "        @btime CUDA.@sync sin.($carr) .+ 1\n",
    "        return nothing\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6e9c41-451f-4c4f-928b-79fc7c6b5023",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Now use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7f29d5e1-5b56-4117-bcc6-76d90a962322",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "using ParameterizedNotebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "48c600c2-747b-4c6d-ab03-063fada5dde9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParameterizedNotebook(\"talk.ipynb\")\n",
       "\u001b[2m‚ñ° ~\u001b[22m\n",
       "  \u001b[2m‚ñ° Julia + Jupyter + GPU = ‚öóÔ∏èüî¨üß¨ü•∞\u001b[22m\n",
       "    \u001b[2m‚ñ° Outline\u001b[22m\n",
       "    \u001b[2m‚ñ° Motivation\u001b[22m\n",
       "    \u001b[2m‚ñ° Install\u001b[22m\n",
       "    \u001b[2m‚ñ° Basic usage\u001b[22m\n",
       "    \u001b[2m‚ñ° Power of Julia (1)\u001b[22m\n",
       "    \u001b[2m‚ñ° Limitations\u001b[22m\n",
       "    \u001b[2m‚ñ° Power of Julia (2)\u001b[22m\n",
       "    \u001b[2m‚ñ° Multi-GPU (single node)\u001b[22m\n",
       "    \u001b[2m‚ñ° Multi-GPU (multiple nodes, elastic)\u001b[22m\n",
       "    \u001b[2m‚ñ° Multi-GPU (multiple nodes, MPI)\u001b[22m\n",
       "    \u001b[2m‚ñ° Multi-GPU (multiple nodes, MPI, notebooks)\u001b[22m\n",
       "      \u001b[1m\u001b[32m‚òí \u001b[39mSome code in a notebook:\u001b[22m\n",
       "        \u001b[1m\u001b[32m‚òí \u001b[39m\u001b[22m\u001b[1m‚Ä¶\u001b[22m\n",
       "      \u001b[2m‚ñ° Now use:\u001b[22m\n",
       "    \u001b[2m‚ñ° Conclusions\u001b[22m"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = ParameterizedNotebook(\"talk.ipynb\", sections=(\"Some code in a notebook:\",))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1f025ce3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  85.074 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 5:\t  82.029 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 6:\t  82.058 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 7:\t  81.968 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 9:\t  81.999 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 3:\t  81.838 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 8:\t  81.818 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 10:\t  81.929 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 4:\t  82.148 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 11:\t  81.808 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 13:\t  82.149 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 12:\t  81.799 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 14:\t  81.948 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 2:\t  82.078 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 16:\t  81.909 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 17:\t  81.908 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 15:\t  81.689 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 18:\t  82.139 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 19:\t  81.988 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 21:\t  81.537 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 20:\t  81.898 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 22:\t  81.698 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 23:\t  81.968 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 25:\t  81.859 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 24:\t  81.508 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 26:\t  81.769 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 27:\t  82.099 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 29:\t  81.728 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 28:\t  81.598 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 30:\t  81.899 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 31:\t  82.019 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 33:\t  81.938 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 32:\t  81.638 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 34:\t  81.758 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 35:\t  81.958 Œºs (37 allocations: 1.91 KiB)\n",
      "      From worker 36:\t  81.908 Œºs (37 allocations: 1.91 KiB)\n"
     ]
    }
   ],
   "source": [
    "nb()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc4015d-af2c-4266-8cc0-ec9f3b384f44",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can put the call to the notebook code directly in a `test_script_2.jl`:\n",
    "```julia\n",
    "#!/bin/bash\n",
    "#SBATCH -C gpu -q regular -A mp107\n",
    "#SBATCH -t 00:05:00 \n",
    "#SBATCH --cpus-per-task 32 --gpus-per-task 1 --ntasks-per-node 4 --nodes 4\n",
    "#=\n",
    "srun /global/u1/m/marius/.julia/juliaup/julia-1.9.3+0.x64.linux.gnu/bin/julia $(scontrol show job $SLURM_JOBID | awk -F= '/Command=/{print $2}')\n",
    "exit 0\n",
    "# =#\n",
    "\n",
    "using MPIClusterManagers, Distributed, CUDA\n",
    "mgr = MPIClusterManagers.start_main_loop(MPIClusterManagers.MPI_TRANSPORT_ALL)\n",
    "\n",
    "nb = ParameterizedNotebook(\"talk.ipynb\", sections=(\"Some code in a notebook:\",))\n",
    "nb()\n",
    "\n",
    "MPIClusterManagers.stop_main_loop(mgr)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbe2c3b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "With some care in the organization of your sections, you can iterate on code in the notebook, even test it in parallel using on-the-fly `ElasticManager` workers, then submit the identical code as an MPI job for larger-scale runs üéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3405e4d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec83b8be-279c-47be-822f-d47f9986adfa",
   "metadata": {},
   "source": [
    "* Julia + Jupyter + GPUs offer powerful scientific workflows\n",
    "* Hopefully I've shared some efficient ways to do this that we've learned\n",
    "* Wishlist\n",
    "    * More robust and easier CUDA.jl task/threading support\n",
    "    * An easy way to use MPI CUDA transport protocol from within Jupyter jobs\n",
    "    * A _multi-node_ GPU monitor, even just a command-line one\n",
    "        * `nvitop`, `btop` (PR), and `gpustat` are some good command line single-node options"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
