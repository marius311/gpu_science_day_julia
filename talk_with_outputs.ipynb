{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3028ecd-a924-40cd-a9fb-919ab9b3f8b4",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".reveal pre code {\n",
       "    max-height: none;\n",
       "    font-size: 90%;\n",
       "}\n",
       ".rise-enabled .text_cell {\n",
       "    font-size: 150%;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "HTML{String}(\"<style>\\n.reveal pre code {\\n    max-height: none;\\n    font-size: 90%;\\n}\\n.rise-enabled .text_cell {\\n    font-size: 150%;\\n}\\n</style>\\n\")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENV[\"LINES\"] = 10;\n",
    "ENV[\"COLUMNS\"] = 80;\n",
    "\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".reveal pre code {\n",
    "    max-height: none;\n",
    "    font-size: 90%;\n",
    "}\n",
    ".rise-enabled .text_cell {\n",
    "    font-size: 150%;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63fb033",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Julia + Jupyter + GPU = âš—ï¸ğŸ”¬ğŸ§¬ğŸ¥°\n",
    "\n",
    "Marius Millea (Project Scientist @ UC Davis in Cosmology)\n",
    "\n",
    "NERSC GPU Science Day, Oct 12, 2023\n",
    "\n",
    "Thanks to: Tim Besard + CUDA.jl/Julia contributors, Johannes Blaschke, Rollin Thomas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf0a160",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "I work on analyzing maps of the Cosmic Microwave Background. Using tiny distortions imprinted by gravitational lensing, we can make maps of where all the dark matter is in the universe. We do so by solving **millions-of-dimensional Bayesian inference problems.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b359fbe",
   "metadata": {},
   "source": [
    "<video controls autoplay loop muted width=\"1800\" height=\"600\" source src=\"kappa_forecast.mp4\" type=\"video/mp4\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df18b60c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Our basic code building blocks are array broadcasts and FFTs, which is perfectly suited for GPU. Our group has been using GPUs since the Cori GPU testbed days.\n",
    "\n",
    "But this talk is not about science, but instead **sharing the workflow we've developed over the last ~5 years.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a614681",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "* Julia + Jupyter + GPU motivation\n",
    "* Julia CUDA Installation\n",
    "* Basic and advanced Julia CUDA usage\n",
    "* Multi-GPU workflows for embarrasingly parallel problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b6d67e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdb55c5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Julia\n",
    "    * interactive but fast\n",
    "    * powerful and flexible\n",
    "    * less boilerplate: code looks like science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6228a33e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Jupyter\n",
    "    * convenient for interactive work\n",
    "    * fast iterative development workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97cfc77",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* GPU\n",
    "    * duh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea42a49-3f34-4061-83f3-d55b3f9edbdc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bfb764",
   "metadata": {},
   "source": [
    "Julia/CUDA install is drop-dead simple. Julia's CUDA package provides compatible binary drivers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1600af3",
   "metadata": {},
   "source": [
    "```shell\n",
    "$ curl -fsSL https://install.julialang.org | sh\n",
    "$ julia\n",
    "pkg> add CUDA # ~2min\n",
    "   Resolving package versions...\n",
    "   Installed CUDA_Driver_jll â”€â”€ v0.6.0+3\n",
    "   Installed LLVMExtra_jll â”€â”€â”€â”€ v0.0.26+0\n",
    "   ...\n",
    "   Installed CUDA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ v5.0.0\n",
    " Downloading artifact: CUDA_Driver\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1318c9b9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "(Easy to select CUDA version _per project_ with e.g. `CUDA.set_runtime_version!(v\"11.4\")`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf466c7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "I recommend this fully native Julia install over using any `modules`, i.e. I don't even have the `gpu` module loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "349bac64-41a5-488a-888c-b9305be35e28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Currently Loaded Modules:\n",
      "  1) craype-x86-milan                        8) cray-mpich/8.1.25\n",
      "  2) libfabric/1.15.2.0                      9) craype/2.7.20\n",
      "  3) craype-network-ofi                     10) gcc/11.2.0\n",
      "  4) xpmem/2.6.2-2.5_2.27__gd067c3f.shasta  11) perftools-base/23.03.0\n",
      "  5) PrgEnv-gnu/8.3.3                       12) cpe/23.03\n",
      "  6) cray-dsmml/0.2.2                       13) xalt/2.10.2\n",
      "  7) cray-libsci/23.02.1.1                  14) cray-python/3.9.13.1   (dev)\n",
      "\n",
      "  Where:\n",
      "   dev:  Development Tools and Programming Languages\n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "; module list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdcec68",
   "metadata": {},
   "source": [
    "This has proven robust across many clusters I've tried."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d35ade1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Checking everything is installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02f49430-09f8-460a-a439-4cf0c3f93b8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e3bb8bd-d25f-4c8b-9b2d-5f13c681bc75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA runtime 12.2, artifact installation\n",
      "CUDA driver 12.2\n",
      "NVIDIA driver 525.105.17, originally for CUDA 12.0\n",
      "\n",
      "CUDA libraries: \n",
      "- CUBLAS: 12.2.5\n",
      "- CURAND: 10.3.3\n",
      "- CUFFT: 11.0.8\n",
      "- CUSOLVER: 11.5.2\n",
      "- CUSPARSE: 12.1.2\n",
      "- CUPTI: 20.0.0\n",
      "- NVML: 12.0.0+525.105.17\n",
      "\n",
      "Julia packages: \n",
      "- CUDA: 5.0.0\n",
      "- CUDA_Driver_jll: 0.6.0+3\n",
      "- CUDA_Runtime_jll: 0.9.2+0\n",
      "\n",
      "Toolchain:\n",
      "- Julia: 1.9.3\n",
      "- LLVM: 14.0.6\n",
      "- PTX ISA support: 3.2, 4.0, 4.1, 4.2, 4.3, 5.0, 6.0, 6.1, 6.3, 6.4, 6.5, 7.0, 7.1, 7.2, 7.3, 7.4, 7.5\n",
      "- Device capability support: sm_37, sm_50, sm_52, sm_53, sm_60, sm_61, sm_62, sm_70, sm_72, sm_75, sm_80, sm_86\n",
      "\n",
      "4 devices:\n",
      "  0: NVIDIA A100-SXM4-40GB (sm_80, 39.389 GiB / 40.000 GiB available)\n",
      "  1: NVIDIA A100-SXM4-40GB (sm_80, 39.389 GiB / 40.000 GiB available)\n",
      "  2: NVIDIA A100-SXM4-40GB (sm_80, 39.389 GiB / 40.000 GiB available)\n",
      "  3: NVIDIA A100-SXM4-40GB (sm_80, 39.389 GiB / 40.000 GiB available)\n"
     ]
    }
   ],
   "source": [
    "CUDA.versioninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5734821-7675-41c5-a547-a02157039f4c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c90f66cf-19ac-4d6d-b4b1-0e6e4e3febd9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000-element Vector{Float64}:\n",
       " 0.20079028039355207\n",
       " 0.2551683713911349\n",
       " 0.07850631788245288\n",
       " â‹®\n",
       " 0.18280216971091756\n",
       " 0.5304310135460691"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = rand(10_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e854be87-f7ec-4206-8410-698b1242cf33",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 0.20079029\n",
       " 0.25516838\n",
       " 0.07850632\n",
       " â‹®\n",
       " 0.18280217\n",
       " 0.53043103"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carr = cu(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "470104fb-7a46-4a34-b6e8-f32e9230b930",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 1.1994438\n",
       " 1.2524083\n",
       " 1.0784256\n",
       " â‹®\n",
       " 1.1817858\n",
       " 1.5059052"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sin.(carr) .+ 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009c3528",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lets benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4e7e49f-a8d9-4656-8e71-c03aa4f95fc0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "using BenchmarkTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d91b6b2-e853-4536-8109-1eec8e4ab8da",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  83.051 Î¼s (41 allocations: 2.00 KiB)\n"
     ]
    }
   ],
   "source": [
    "@btime CUDA.@sync sin.(carr) .+ 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "321dcf26-1d65-49c5-8c25-79df78c8d97d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  68.599 ms (6 allocations: 76.29 MiB)\n"
     ]
    }
   ],
   "source": [
    "@btime sin.(arr) .+ 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "970e23ed-a0ef-401e-a22f-df98921f7ba8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiler ran for 326.63 Âµs, capturing 11 events.\n",
      "\n",
      "Host-side activity: calling CUDA APIs took 61.75 Âµs (18.91% of the trace)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚\u001b[1m Time (%) \u001b[0mâ”‚\u001b[1m     Time \u001b[0mâ”‚\u001b[1m Calls \u001b[0mâ”‚\u001b[1m Avg time \u001b[0mâ”‚\u001b[1m Min time \u001b[0mâ”‚\u001b[1m Max time \u001b[0mâ”‚\u001b[1m Name                    \u001b[0mâ”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚    9.56% â”‚\u001b[31m 31.23 Âµs \u001b[0mâ”‚     1 â”‚ 31.23 Âµs â”‚ 31.23 Âµs â”‚ 31.23 Âµs â”‚\u001b[1m cuLaunchKernel          \u001b[0mâ”‚\n",
      "â”‚    7.66% â”‚ 25.03 Âµs â”‚     1 â”‚ 25.03 Âµs â”‚ 25.03 Âµs â”‚ 25.03 Âµs â”‚ cuMemAllocFromPoolAsync â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "Device-side activity: GPU was busy for 81.54 Âµs (24.96% of the trace)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "â”‚\u001b[1m Time (%) \u001b[0mâ”‚\u001b[1m     Time \u001b[0mâ”‚\u001b[1m Calls \u001b[0mâ”‚\u001b[1m Avg time \u001b[0mâ”‚\u001b[1m Min time \u001b[0mâ”‚\u001b[1m Max time \u001b[0mâ”‚\u001b[1m Name                                                                                                        \u001b[0m â‹¯\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "â”‚   24.96% â”‚\u001b[31m 81.54 Âµs \u001b[0mâ”‚     1 â”‚ 81.54 Âµs â”‚ 81.54 Âµs â”‚ 81.54 Âµs â”‚\u001b[1m _Z16broadcast_kernel15CuKernelContext13CuDeviceArrayI7Float32Li1ELi1EE11BroadcastedI12CuArrayStyleILi1EE5Tup\u001b[0m â‹¯\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\u001b[36m                                                                                                                                                               1 column omitted\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "CUDA.@profile sin.(carr) .+ 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4506ce3c-0868-4eea-bbc7-b3004d589e60",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Power of Julia (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fadaaea",
   "metadata": {},
   "source": [
    "In Julia, you can easily put many arbitrary objects on GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "382f1b13-28ee-4a9d-b38a-bec483b0254e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "struct Point{T}\n",
    "    x :: T\n",
    "    y :: T\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35ae9488-9d0b-4c43-8012-5b476c2bdfed",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100-element CuArray{Point{Float64}, 1, CUDA.Mem.DeviceBuffer}:\n",
       " Point{Float64}(0.8490008946627912, 0.48658520886875856)\n",
       " Point{Float64}(0.06977616429006461, 0.2501647436222665)\n",
       " Point{Float64}(0.6924522464442648, 0.2656146874146924)\n",
       " â‹®\n",
       " Point{Float64}(0.8748131265382463, 0.2480993353552592)\n",
       " Point{Float64}(0.49503190701987954, 0.13355513219798332)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = Point.(rand(100), rand(100))\n",
    "carr = cu(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9555b43",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In e.g. Jax/PyTorch/TF, the only things you can stick inside of CUDA arrays are Int/Float/Complex. In Julia, anything with a static memory layout is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "528d1f95-c491-4e96-94cb-e0c976c511d6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "distance_from_origin (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_from_origin(p::Point) = sqrt(p.x^2 + p.y^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba152df7-0d00-477c-9eb1-ddfe355b4cfe",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100-element CuArray{Float64, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 0.9785538741572041\n",
       " 0.2597135191988057\n",
       " 0.7416476763100615\n",
       " â‹®\n",
       " 0.9093136348737674\n",
       " 0.5127314719267381"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_from_origin.(carr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbac985e-bc4e-4579-a709-ab0b37399fea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "832d529f-7dfd-48f4-a5ae-eb80b08c3fa9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "distance_from_origin_bad (generic function with 1 method)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function distance_from_origin_bad(p::Point)\n",
    "    sqrt(sum([p.x^2, p.y^2]))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "727c6377-d9d2-42e2-a8b4-45cef1ed4db8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "InvalidIRError: compiling MethodInstance for (::GPUArrays.var\"#broadcast_kernel#32\")(::CUDA.CuKernelContext, ::CuDeviceVector{Float64, 1}, ::Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{1}, Tuple{Base.OneTo{Int64}}, typeof(distance_from_origin_bad), Tuple{Base.Broadcast.Extruded{CuDeviceVector{Point{Float64}, 1}, Tuple{Bool}, Tuple{Int64}}}}, ::Int64) resulted in invalid LLVM IR\n\u001b[31mReason: unsupported call through a literal pointer\u001b[39m\u001b[31m (call to ijl_alloc_array_1d)\u001b[39m\nStacktrace:\n  [1] \u001b[0m\u001b[1mArray\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mboot.jl:477\u001b[24m\u001b[39m\n  [2] \u001b[0m\u001b[1mArray\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mboot.jl:486\u001b[24m\u001b[39m\n  [3] \u001b[0m\u001b[1msimilar\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mabstractarray.jl:884\u001b[24m\u001b[39m\n  [4] \u001b[0m\u001b[1msimilar\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mabstractarray.jl:883\u001b[24m\u001b[39m\n  [5] \u001b[0m\u001b[1m_array_for\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4marray.jl:671\u001b[24m\u001b[39m\n  [6] \u001b[0m\u001b[1m_array_for\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4marray.jl:674\u001b[24m\u001b[39m\n  [7] \u001b[0m\u001b[1mvect\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4marray.jl:126\u001b[24m\u001b[39m\n  [8] \u001b[0m\u001b[1mdistance_from_origin_bad\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mIn[19]:2\u001b[24m\u001b[39m\n  [9] \u001b[0m\u001b[1m_broadcast_getindex_evalf\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mbroadcast.jl:683\u001b[24m\u001b[39m\n [10] \u001b[0m\u001b[1m_broadcast_getindex\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mbroadcast.jl:656\u001b[24m\u001b[39m\n [11] \u001b[0m\u001b[1mgetindex\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mbroadcast.jl:610\u001b[24m\u001b[39m\n [12] \u001b[0m\u001b[1mbroadcast_kernel\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m~/.julia/packages/GPUArrays/EZkix/src/host/\u001b[39m\u001b[90m\u001b[4mbroadcast.jl:64\u001b[24m\u001b[39m\n\u001b[36m\u001b[1mHint\u001b[22m\u001b[39m\u001b[36m: catch this exception as `err` and call `code_typed(err; interactive = true)` to introspect the erronous code with Cthulhu.jl\u001b[39m",
     "output_type": "error",
     "traceback": [
      "InvalidIRError: compiling MethodInstance for (::GPUArrays.var\"#broadcast_kernel#32\")(::CUDA.CuKernelContext, ::CuDeviceVector{Float64, 1}, ::Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{1}, Tuple{Base.OneTo{Int64}}, typeof(distance_from_origin_bad), Tuple{Base.Broadcast.Extruded{CuDeviceVector{Point{Float64}, 1}, Tuple{Bool}, Tuple{Int64}}}}, ::Int64) resulted in invalid LLVM IR\n\u001b[31mReason: unsupported call through a literal pointer\u001b[39m\u001b[31m (call to ijl_alloc_array_1d)\u001b[39m\nStacktrace:\n  [1] \u001b[0m\u001b[1mArray\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mboot.jl:477\u001b[24m\u001b[39m\n  [2] \u001b[0m\u001b[1mArray\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mboot.jl:486\u001b[24m\u001b[39m\n  [3] \u001b[0m\u001b[1msimilar\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mabstractarray.jl:884\u001b[24m\u001b[39m\n  [4] \u001b[0m\u001b[1msimilar\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mabstractarray.jl:883\u001b[24m\u001b[39m\n  [5] \u001b[0m\u001b[1m_array_for\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4marray.jl:671\u001b[24m\u001b[39m\n  [6] \u001b[0m\u001b[1m_array_for\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4marray.jl:674\u001b[24m\u001b[39m\n  [7] \u001b[0m\u001b[1mvect\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4marray.jl:126\u001b[24m\u001b[39m\n  [8] \u001b[0m\u001b[1mdistance_from_origin_bad\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mIn[19]:2\u001b[24m\u001b[39m\n  [9] \u001b[0m\u001b[1m_broadcast_getindex_evalf\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mbroadcast.jl:683\u001b[24m\u001b[39m\n [10] \u001b[0m\u001b[1m_broadcast_getindex\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mbroadcast.jl:656\u001b[24m\u001b[39m\n [11] \u001b[0m\u001b[1mgetindex\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mbroadcast.jl:610\u001b[24m\u001b[39m\n [12] \u001b[0m\u001b[1mbroadcast_kernel\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m~/.julia/packages/GPUArrays/EZkix/src/host/\u001b[39m\u001b[90m\u001b[4mbroadcast.jl:64\u001b[24m\u001b[39m\n\u001b[36m\u001b[1mHint\u001b[22m\u001b[39m\u001b[36m: catch this exception as `err` and call `code_typed(err; interactive = true)` to introspect the erronous code with Cthulhu.jl\u001b[39m",
      "",
      "Stacktrace:",
      "  [1] check_ir(job::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams}, args::LLVM.Module)",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/2mJjc/src/validation.jl:147",
      "  [2] macro expansion",
      "    @ ~/.julia/packages/GPUCompiler/2mJjc/src/driver.jl:440 [inlined]",
      "  [3] macro expansion",
      "    @ ~/.julia/packages/TimerOutputs/RsWnF/src/TimerOutput.jl:253 [inlined]",
      "  [4] macro expansion",
      "    @ ~/.julia/packages/GPUCompiler/2mJjc/src/driver.jl:439 [inlined]",
      "  [5] emit_llvm(job::GPUCompiler.CompilerJob; libraries::Bool, toplevel::Bool, optimize::Bool, cleanup::Bool, only_entry::Bool, validate::Bool)",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/2mJjc/src/utils.jl:92",
      "  [6] codegen(output::Symbol, job::GPUCompiler.CompilerJob; libraries::Bool, toplevel::Bool, optimize::Bool, cleanup::Bool, strip::Bool, validate::Bool, only_entry::Bool, parent_job::Nothing)",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/2mJjc/src/driver.jl:129",
      "  [7] codegen",
      "    @ ~/.julia/packages/GPUCompiler/2mJjc/src/driver.jl:110 [inlined]",
      "  [8] compile(target::Symbol, job::GPUCompiler.CompilerJob; libraries::Bool, toplevel::Bool, optimize::Bool, cleanup::Bool, strip::Bool, validate::Bool, only_entry::Bool)",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/2mJjc/src/driver.jl:106",
      "  [9] compile",
      "    @ ~/.julia/packages/GPUCompiler/2mJjc/src/driver.jl:98 [inlined]",
      " [10] #1042",
      "    @ ~/.julia/packages/CUDA/nbRJk/src/compiler/compilation.jl:166 [inlined]",
      " [11] JuliaContext(f::CUDA.var\"#1042#1045\"{GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams}})",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/2mJjc/src/driver.jl:47",
      " [12] compile(job::GPUCompiler.CompilerJob)",
      "    @ CUDA ~/.julia/packages/CUDA/nbRJk/src/compiler/compilation.jl:165",
      " [13] actual_compilation(cache::Dict{Any, CuFunction}, src::Core.MethodInstance, world::UInt64, cfg::GPUCompiler.CompilerConfig{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams}, compiler::typeof(CUDA.compile), linker::typeof(CUDA.link))",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/2mJjc/src/execution.jl:125",
      " [14] cached_compilation(cache::Dict{Any, CuFunction}, src::Core.MethodInstance, cfg::GPUCompiler.CompilerConfig{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams}, compiler::Function, linker::Function)",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/2mJjc/src/execution.jl:103",
      " [15] macro expansion",
      "    @ ~/.julia/packages/CUDA/nbRJk/src/compiler/execution.jl:323 [inlined]",
      " [16] macro expansion",
      "    @ ./lock.jl:267 [inlined]",
      " [17] cufunction(f::GPUArrays.var\"#broadcast_kernel#32\", tt::Type{Tuple{CUDA.CuKernelContext, CuDeviceVector{Float64, 1}, Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{1}, Tuple{Base.OneTo{Int64}}, typeof(distance_from_origin_bad), Tuple{Base.Broadcast.Extruded{CuDeviceVector{Point{Float64}, 1}, Tuple{Bool}, Tuple{Int64}}}}, Int64}}; kwargs::Base.Pairs{Symbol, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})",
      "    @ CUDA ~/.julia/packages/CUDA/nbRJk/src/compiler/execution.jl:318",
      " [18] cufunction",
      "    @ ~/.julia/packages/CUDA/nbRJk/src/compiler/execution.jl:315 [inlined]",
      " [19] macro expansion",
      "    @ ~/.julia/packages/CUDA/nbRJk/src/compiler/execution.jl:104 [inlined]",
      " [20] #launch_heuristic#1087",
      "    @ ~/.julia/packages/CUDA/nbRJk/src/gpuarrays.jl:17 [inlined]",
      " [21] launch_heuristic",
      "    @ ~/.julia/packages/CUDA/nbRJk/src/gpuarrays.jl:15 [inlined]",
      " [22] _copyto!",
      "    @ ~/.julia/packages/GPUArrays/EZkix/src/host/broadcast.jl:70 [inlined]",
      " [23] copyto!",
      "    @ ~/.julia/packages/GPUArrays/EZkix/src/host/broadcast.jl:51 [inlined]",
      " [24] copy",
      "    @ ~/.julia/packages/GPUArrays/EZkix/src/host/broadcast.jl:42 [inlined]",
      " [25] materialize(bc::Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{1}, Nothing, typeof(distance_from_origin_bad), Tuple{CuArray{Point{Float64}, 1, CUDA.Mem.DeviceBuffer}}})",
      "    @ Base.Broadcast ./broadcast.jl:873",
      " [26] top-level scope",
      "    @ In[20]:1"
     ]
    }
   ],
   "source": [
    "distance_from_origin_bad.(carr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807107d4-73c8-473a-9d13-737c2311897c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Limitations on code in functions that will be compiled for GPU:\n",
    "* No calls to CPU functions\n",
    "   * E.g. creating Arrays (use StaticArrays.jl instead)\n",
    "* No _dynamic dispatch_\n",
    "   * Code should be _type stable_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cae2fe-6444-479e-a1f6-98acb5800ba5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Power of Julia (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf0865",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You can also directly write kernels in Julia, giving the full power and flexibility of CUDA kernel programming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1ad299e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "my_kernel (generic function with 1 method)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function my_kernel(carr_out, carr)   \n",
    "    start = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n",
    "    stride = blockDim().x * gridDim().x\n",
    "    len = length(carr)\n",
    "    for i = start:stride:len  # \"grid-stride\" loop\n",
    "        carr_out[i] = sin(carr[i]) + 1\n",
    "    end\n",
    "    return\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "635c6648",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "carr = cu(rand(10_000_000))\n",
    "carr_out = similar(carr);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ab21fce",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CUDA.HostKernel for my_kernel(CuDeviceVector{Float32, 1}, CuDeviceVector{Float32, 1})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@cuda threads=256 my_kernel(carr_out, carr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce0793f0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 1.264759\n",
       " 1.3415114\n",
       " 1.0137947\n",
       " â‹®\n",
       " 1.7018087\n",
       " 1.215888"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carr_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0c5d3c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "See [Kernel Programming](https://cuda.juliagpu.org/stable/api/kernel/) for full list of CUDA.jl kernel programming capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ae7b4e-0547-4f11-83aa-94433a2af38d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-GPU (single node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2ead7fe-334b-4c1a-a6d6-909dd01d7ff5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CUDA.DeviceIterator() for 4 devices:\n",
       "0. NVIDIA A100-SXM4-40GB\n",
       "1. NVIDIA A100-SXM4-40GB\n",
       "2. NVIDIA A100-SXM4-40GB\n",
       "3. NVIDIA A100-SXM4-40GB"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUDA.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc5b554c-33e9-4512-b0e3-41014eb3106b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CuDevice(0): NVIDIA A100-SXM4-40GB"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUDA.device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e3976cb-3305-44ac-97b7-3dd7363646e7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CuDevice(1): NVIDIA A100-SXM4-40GB"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUDA.device!(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65eaa3e0-3097-44df-8402-c43a20c88d8e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  85.705 Î¼s (41 allocations: 2.00 KiB)\n"
     ]
    }
   ],
   "source": [
    "arr = rand(10_000_000)\n",
    "carr = cu(arr)\n",
    "@btime CUDA.@sync sin.(carr) .+ 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbee4a7-f1fc-43c3-a2ac-95e40758bc07",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "CUDA.jl does its own memory management, so before switching back to GPU 0, give back memory (don't usually have to think about this unless you use the same GPU from multiple processes, which for the purpose of this demo I do):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0da831ed-5f8e-4886-b7d6-1fbc7acff2eb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "GC.gc()\n",
    "CUDA.reclaim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a29fbf8c-f564-4d65-a4d9-b73321fa11c7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CuDevice(0): NVIDIA A100-SXM4-40GB"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUDA.device!(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e376af19",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can use multiple GPUs via Julia processes, tasks, or threads. \n",
    "\n",
    "The most robust and easy way I have found (as of 2023), which I recommend starting with, is per-_process_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2f51e76-6517-4a49-95e2-5a3459d63a81",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "using Distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d940d90-6514-4d8d-9e3b-07da5a60d32f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Int64}:\n",
       " 2\n",
       " 3\n",
       " 4"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addprocs(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "49108a26-d4fb-4fa4-905c-a71a71d8f5a2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@everywhere using CUDA, BenchmarkTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4094404e-7f67-488c-ba27-71270b8284b2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, CuDevice(0))\n",
      "      From worker 3:\t(3, CuDevice(0))\n",
      "      From worker 2:\t(2, CuDevice(0))\n",
      "      From worker 4:\t(4, CuDevice(0))\n"
     ]
    }
   ],
   "source": [
    "@everywhere procs() println((myid(), CUDA.device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9202be9-166c-4416-abf6-57b9bdd9da47",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@everywhere procs() CUDA.device!(myid()-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "77aa9ba8-faaf-42bd-8c58-86c4423a468b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, CuDevice(0))\n",
      "      From worker 2:\t(2, CuDevice(1))\n",
      "      From worker 3:\t(3, CuDevice(2))\n",
      "      From worker 4:\t(4, CuDevice(3))\n"
     ]
    }
   ],
   "source": [
    "@everywhere procs() println((myid(), CUDA.device()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119909c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lets run our benchmark in parallel across all GPUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7edbb80-4e02-462c-b2ca-127c35b14130",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  85.255 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 3:\t  81.938 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 2:\t  82.238 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 4:\t  81.597 Î¼s (37 allocations: 1.91 KiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4-element Vector{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}:\n",
       " Float32[1.8228395, 1.0992318, 1.759596, 1.8266902, 1.1692553, 1.3392247, 1.7539189, 1.7294312, 1.705107, 1.6065087  â€¦  1.4714574, 1.0195016, 1.0635127, 1.5888524, 1.7992377, 1.333357, 1.2087038, 1.0803039, 1.7202525, 1.5920569]\n",
       " Float32[1.8228395, 1.0992318, 1.759596, 1.8266902, 1.1692553, 1.3392247, 1.7539189, 1.7294312, 1.705107, 1.6065087  â€¦  1.4714574, 1.0195016, 1.0635127, 1.5888524, 1.7992377, 1.333357, 1.2087038, 1.0803039, 1.7202525, 1.5920569]\n",
       " Float32[1.8228395, 1.0992318, 1.759596, 1.8266902, 1.1692553, 1.3392247, 1.7539189, 1.7294312, 1.705107, 1.6065087  â€¦  1.4714574, 1.0195016, 1.0635127, 1.5888524, 1.7992377, 1.333357, 1.2087038, 1.0803039, 1.7202525, 1.5920569]\n",
       " Float32[1.8228395, 1.0992318, 1.759596, 1.8266902, 1.1692553, 1.3392247, 1.7539189, 1.7294312, 1.705107, 1.6065087  â€¦  1.4714574, 1.0195016, 1.0635127, 1.5888524, 1.7992377, 1.333357, 1.2087038, 1.0803039, 1.7202525, 1.5920569]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let\n",
    "    carr = cu(rand(10_000_000))\n",
    "    pmap(WorkerPool(procs()), 1:4) do i\n",
    "        @btime CUDA.@sync sin.($carr) .+ 1\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7edc333-7c3a-4760-95e0-b7e10f2789c8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note, `carr` was defined and moved to GPU on the master process. Julia automatically sent it to the worker GPUs, then automatically sent the results back to the master GPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1c3999-845c-4b3e-8e54-a2215aa3801b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In doing so, the array passed through CPU memory, so its not the most efficient (but its the easiest)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fafedca-7e29-4c4d-86a8-9edcca2e4718",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To go straight GPU-to-GPU, you can use _unified memory_ on a single-node, or CUDA MPI transport (later this talk)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bfa320-711a-487f-92ef-526728c517c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-GPU (multiple nodes, elastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cbe338f9-afc9-4648-9b27-f59c1c21e543",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "using ClusterManagers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d8bf1b4a-344f-4d43-b485-ae04335339cc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "em = ElasticManager(\n",
    "    # Perlmutter specific â†“\n",
    "    addr = IPv4(first(filter(!isnothing, match.(r\"inet (.*)/.*hsn0\", readlines(`ip a show`)))).captures[1]),\n",
    "    port = 0\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4f2284ef-47ab-40fa-8e11-28f2c4bdefba",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticManager:\n",
       "  Active workers : [ 5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36]\n",
       "  Number of workers to be added  : 0\n",
       "  Terminated workers : []\n",
       "  Worker connect command : \n",
       "    /global/u1/m/marius/.julia/juliaup/julia-1.9.3+0.x64.linux.gnu/bin/julia --project=/global/u1/m/marius/work/gpu_science_day_julia/Project.toml -e 'using ClusterManagers; ClusterManagers.elastic_worker(\"6Ty6RCu5sIy5CedV\",\"10.249.6.77\",35449)'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44088681-107f-4268-8b5b-d1e7c173fd98",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now submit a job, e.g. with:\n",
    "```bash\n",
    "salloc -C gpu -q regular -t 00:30:00 --cpus-per-task 32  --gpus-per-task 1 --ntasks-per-node 4 --nodes 8 -A mp107\n",
    "```\n",
    "then run the \"worker connect command\" printed above (could also do all-in-one as a batch job)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ac85b2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "With more GPUs across different nodes, its more complex to assign one unique GPU to each process. Instead we can use this utility function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "582fa7c0-1f84-46b7-852e-5afc9df059ee",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "using CUDADistributedTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a6d35638-7a08-4b9f-88ae-787a54b4b781",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mâ”Œ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mProcesses (36):\n",
      "\u001b[36m\u001b[1mâ”‚ \u001b[22m\u001b[39m (myid = 1, host = nid001293, device = CuDevice(0): NVIDIA A100-SXM4-40GB 1c40175b))\n",
      "\u001b[36m\u001b[1mâ”‚ \u001b[22m\u001b[39m (myid = 2, host = nid001293, device = CuDevice(1): NVIDIA A100-SXM4-40GB f179efe2))\n",
      "\u001b[36m\u001b[1mâ”‚ \u001b[22m\u001b[39m (myid = 3, host = nid001293, device = CuDevice(2): NVIDIA A100-SXM4-40GB 36d32866))\n",
      "\u001b[36m\u001b[1mâ”‚ \u001b[22m\u001b[39m (myid = 4, host = nid001293, device = CuDevice(3): NVIDIA A100-SXM4-40GB 634451b9))\n",
      "\u001b[36m\u001b[1mâ”‚ \u001b[22m\u001b[39m (myid = 5, host = nid002532, device = CuDevice(0): NVIDIA A100-SXM4-40GB 892d65ed))\n",
      "\u001b[36m\u001b[1mâ”‚ \u001b[22m\u001b[39m (myid = 6, host = nid002532, device = CuDevice(0): NVIDIA A100-SXM4-40GB 0212ac25))\n",
      "\u001b[36m\u001b[1mâ”‚ \u001b[22m\u001b[39m (myid = 7, host = nid002532, device = CuDevice(0): NVIDIA A100-SXM4-40GB 9f1b5f73))\n",
      "\u001b[36m\u001b[1mâ”‚ \u001b[22m\u001b[39m (myid = 8, host = nid002532, device = CuDevice(0): NVIDIA A100-SXM4-40GB b9ac9c36))\n",
      "\u001b[36m\u001b[1mâ”‚ \u001b[22m\u001b[39m (myid = 9, host = nid002536, device = CuDevice(0): NVIDIA A100-SXM4-40GB 1ffb4f18))\n",
      "\u001b[36m\u001b[1mâ”‚ \u001b[22m\u001b[39m (myid = 10, host = nid002536, device = CuDevice(0): NVIDIA A100-SXM4-40GB a25217d5))\n",
      "\u001b[36m\u001b[1mâ”‚ \u001b[22m\u001b[39m (myid = 11, host = nid002536, device = CuDevice(0): NVIDIA A100-SXM4-40GB 2ad12529))\n",
      "\u001b[36m\u001b[1mâ”‚ \u001b[22m\u001b[39m (myid = 12, host = nid002536, device = CuDevice(0): NVIDIA A100-SXM4-40GB 91817c8d))\n",
      "\u001b[36m\u001b[1mâ”‚ \u001b[22m\u001b[39m (myid = 13, host = nid003320, device = CuDevice(0): NVIDIA A100-SXM4-40GB 6f8ab1df))\n",
      "\u001b[36m\u001b[1mâ”‚ \u001b[22m\u001b[39m (myid = 14, host = nid003320, device = CuDevice(0): NVIDIA A100-SXM4-40GB 014e077e))\n",
      "\u001b[36m\u001b[1mâ”‚ \u001b[22m\u001b[39m (myid = 15, host = nid003320, device = CuDevice(0): NVIDIA A100-SXM4-40GB 38a58e41))\n",
      "\u001b[36m\u001b[1mâ”‚ \u001b[22m\u001b[39m (myid = 16, host = nid003320, device = CuDevice(0): NVIDIA A100-SXM4-40GB a860a000))\n",
      "\u001b[36m\u001b[1mâ”‚ \u001b[22m\u001b[39m (myid = 17, host = nid003316, device = CuDevice(0): NVIDIA A100-SXM4-40GB fdfe719c))\n",
      "\u001b[36m\u001b[1mâ”‚ \u001b[22m\u001b[39m (myid = 18, host = nid003316, device = CuDevice(0): NVIDIA A100-SXM4-40GB 547b9f5c))\n",
      "\u001b[36m\u001b[1mâ”‚ \u001b[22m\u001b[39m (myid = 19, host = nid003316, device = CuDevice(0): NVIDIA A100-SXM4-40GB ee15a3a3))\n",
      "\u001b[36m\u001b[1mâ”‚ \u001b[22m\u001b[39m (myid = 20, host = nid003316, device = CuDevice(0): NVIDIA A100-SXM4-40GB 32342d63))\n",
      "\u001b[36m\u001b[1mâ”‚ \u001b[22m\u001b[39m (myid = 21, host = nid002533, device = CuDevice(0): NVIDIA A100-SXM4-40GB f5695274))\n",
      "\u001b[36m\u001b[1mâ”‚ \u001b[22m\u001b[39m (myid = 22, host = nid002533, device = CuDevice(0): NVIDIA A100-SXM4-40GB 4cdbb673))\n",
      "\u001b[36m\u001b[1mâ”‚ \u001b[22m\u001b[39m (myid = 23, host = nid002533, device = CuDevice(0): NVIDIA A100-SXM4-40GB 0130c469))\n",
      "\u001b[36m\u001b[1mâ”‚ \u001b[22m\u001b[39m (myid = 24, host = nid003317, device = CuDevice(0): NVIDIA A100-SXM4-40GB 5aeab6da))\n",
      "\u001b[36m\u001b[1mâ”‚ \u001b[22m\u001b[39m (myid = 25, host = nid003317, device = CuDevice(0): NVIDIA A100-SXM4-40GB 44081cfa))\n",
      "\u001b[36m\u001b[1mâ”‚ \u001b[22m\u001b[39m (myid = 26, host = nid003317, device = CuDevice(0): NVIDIA A100-SXM4-40GB 0a4aa27d))\n",
      "\u001b[36m\u001b[1mâ”‚ \u001b[22m\u001b[39m (myid = 27, host = nid003317, device = CuDevice(0): NVIDIA A100-SXM4-40GB 06c81dc5))\n",
      "\u001b[36m\u001b[1mâ”‚ \u001b[22m\u001b[39m (myid = 28, host = nid002533, device = CuDevice(0): NVIDIA A100-SXM4-40GB 20734e54))\n",
      "\u001b[36m\u001b[1mâ”‚ \u001b[22m\u001b[39m (myid = 29, host = nid003313, device = CuDevice(0): NVIDIA A100-SXM4-40GB 60aa76e5))\n",
      "\u001b[36m\u001b[1mâ”‚ \u001b[22m\u001b[39m (myid = 30, host = nid003313, device = CuDevice(0): NVIDIA A100-SXM4-40GB 249cdaab))\n",
      "\u001b[36m\u001b[1mâ”‚ \u001b[22m\u001b[39m (myid = 31, host = nid003313, device = CuDevice(0): NVIDIA A100-SXM4-40GB da05c388))\n",
      "\u001b[36m\u001b[1mâ”‚ \u001b[22m\u001b[39m (myid = 32, host = nid003313, device = CuDevice(0): NVIDIA A100-SXM4-40GB dc1c5e9d))\n",
      "\u001b[36m\u001b[1mâ”‚ \u001b[22m\u001b[39m (myid = 33, host = nid003321, device = CuDevice(0): NVIDIA A100-SXM4-40GB 3b627630))\n",
      "\u001b[36m\u001b[1mâ”‚ \u001b[22m\u001b[39m (myid = 34, host = nid003321, device = CuDevice(0): NVIDIA A100-SXM4-40GB 76cf68e2))\n",
      "\u001b[36m\u001b[1mâ”‚ \u001b[22m\u001b[39m (myid = 35, host = nid003321, device = CuDevice(0): NVIDIA A100-SXM4-40GB b2cb91b4))\n",
      "\u001b[36m\u001b[1mâ”” \u001b[22m\u001b[39m (myid = 36, host = nid003321, device = CuDevice(0): NVIDIA A100-SXM4-40GB 4f421754))\n"
     ]
    }
   ],
   "source": [
    "CUDADistributedTools.assign_GPU_workers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce91d09",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's run parallel benchmarks again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6c79cdcd-2c28-4c03-aa33-9aac7c239fbe",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@everywhere using CUDA, BenchmarkTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "199904a6-ccb2-4bcb-b358-b9a9830fdc6a",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  85.174 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 3:\t  81.828 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 4:\t  81.768 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 2:\t  82.759 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 5:\t  81.707 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 8:\t  81.758 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 7:\t  81.897 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 9:\t  82.840 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 10:\t  81.388 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 12:\t  81.898 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 11:\t  81.638 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 6:\t  83.030 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 15:\t  81.608 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 13:\t  83.251 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 16:\t  82.169 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 14:\t  83.382 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 19:\t  82.439 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 18:\t  82.630 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 22:\t  81.197 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 26:\t  82.680 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 17:\t  83.001 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 21:\t  81.497 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 29:\t  81.617 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 20:\t  82.448 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 24:\t  81.669 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 25:\t  82.230 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 33:\t  81.507 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 32:\t  81.618 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 30:\t  81.447 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 35:\t  81.908 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 31:\t  81.718 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 27:\t  82.640 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 34:\t  81.577 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 28:\t  82.680 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 23:\t  81.618 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 36:\t  81.768 Î¼s (37 allocations: 1.91 KiB)\n"
     ]
    }
   ],
   "source": [
    "let\n",
    "    carr = cu(rand(10_000_000))\n",
    "    pmap(WorkerPool(procs()), 1:nprocs()) do i\n",
    "        @btime CUDA.@sync sin.($carr) .+ 1\n",
    "        return nothing\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e7a87f-4286-4a7e-a087-304ed97b32da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-GPU (multiple nodes, MPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cbf52d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Installing MPI for Julia and configuring:\n",
    "```julia\n",
    "pkg> add MPI MPIPreferences\n",
    "\n",
    "julia> MPIPreferences.use_system_binary(;vendor=\"cray\", mpiexec=\"srun\") # <- options are Perlmutter specific\n",
    "\n",
    "â”Œ Info: MPI implementation identified\n",
    "â”‚   libmpi = \"libmpi_gnu_91.so\"\n",
    "â”‚   version_string = \"MPI VERSION    : CRAY MPICH version 8.1.25.17 (ANL base 3.4a2)\\nMPI BUILD INFO : Sun Feb 26 15:15 2023 (git hash aecd99f)\\n\"\n",
    "â”‚   impl = \"CrayMPICH\"\n",
    "â”‚   version = v\"8.1.25\"\n",
    "â””   abi = \"MPICH\"\n",
    "â”Œ Info: MPIPreferences changed\n",
    "â”‚   binary = \"system\"\n",
    "â”‚   libmpi = \"libmpi_gnu_91.so\"\n",
    "â”‚   abi = \"MPICH\"\n",
    "â”‚   mpiexec = \"srun\"\n",
    "â”‚   preloads =\n",
    "â”‚    1-element Vector{String}:\n",
    "â”‚     \"libmpi_gtl_cuda.so\"\n",
    "â””   preloads_env_switch = \"MPICH_GPU_SUPPORT_ENABLED\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5520ab5e-f687-41a3-bc12-2293241040c2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "(This works thanks to among others NERSC's Johannes Blaschke's contributions to MPI.jl) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f5780f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "You can put SLURM script and Julia script in one file \n",
    "`test_script.jl`:\n",
    "\n",
    "```julia\n",
    "#!/bin/bash\n",
    "#SBATCH -C gpu -q regular -A mp107\n",
    "#SBATCH -t 00:05:00 \n",
    "#SBATCH --cpus-per-task 32 --gpus-per-task 1 --ntasks-per-node 4 --nodes 4\n",
    "#=\n",
    "srun /global/u1/m/marius/.julia/juliaup/julia-1.9.3+0.x64.linux.gnu/bin/julia $(scontrol show job $SLURM_JOBID | awk -F= '/Command=/{print $2}')\n",
    "exit 0\n",
    "# =#\n",
    "\n",
    "using MPIClusterManagers, Distributed, CUDA, BenchmarkTools\n",
    "mgr = MPIClusterManagers.start_main_loop(MPIClusterManagers.MPI_TRANSPORT_ALL)\n",
    "\n",
    "let\n",
    "    carr = cu(rand(10_000_000))\n",
    "    pmap(WorkerPool(procs()), 1:nprocs()) do i\n",
    "        @btime CUDA.@sync sin.($carr) .+ 1\n",
    "    end\n",
    "end\n",
    "\n",
    "MPIClusterManagers.stop_main_loop(mgr)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd5c933",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Then `sbatch test_script.jl`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9775252-0ffd-47b9-9d3b-f161257bd22b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Here, movement of memory between GPUs will happen via CUDA MPI transport ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da0ab8d-683f-4af1-8df5-2950d74550af",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-GPU (multiple nodes, MPI, notebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1bb015-b678-49e3-82f8-ec8a86e42c07",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Some code in a notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7e99065e-7209-4338-8f38-a1a389b5e238",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  85.515 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 5:\t  81.698 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 6:\t  81.728 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 2:\t  81.918 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 8:\t  81.697 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 7:\t  81.637 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 9:\t  81.338 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 10:\t  81.197 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 3:\t  81.867 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 13:\t  81.968 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 4:\t  81.838 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 11:\t  81.668 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 12:\t  82.049 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 15:\t  80.727 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 14:\t  81.378 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 16:\t  82.159 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 17:\t  81.297 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 18:\t  81.277 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 20:\t  81.357 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 21:\t  81.899 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 19:\t  81.637 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 23:\t  81.597 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 22:\t  81.558 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 25:\t  81.738 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 24:\t  81.587 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 26:\t  81.558 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 28:\t  81.688 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 27:\t  81.808 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 29:\t  81.798 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 30:\t  82.329 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 31:\t  81.658 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 33:\t  81.668 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 32:\t  81.788 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 35:\t  81.457 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 34:\t  81.778 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 36:\t  81.838 Î¼s (37 allocations: 1.91 KiB)\n"
     ]
    }
   ],
   "source": [
    "let\n",
    "    carr = cu(rand(10_000_000))\n",
    "    pmap(WorkerPool(procs()), 1:nprocs()) do i\n",
    "        @btime CUDA.@sync sin.($carr) .+ 1\n",
    "        return nothing\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6e9c41-451f-4c4f-928b-79fc7c6b5023",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Now use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7f29d5e1-5b56-4117-bcc6-76d90a962322",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "using ParameterizedNotebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "48c600c2-747b-4c6d-ab03-063fada5dde9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParameterizedNotebook(\"talk.ipynb\")\n",
       "\u001b[2mâ–¡ ~\u001b[22m\n",
       "  \u001b[2mâ–¡ Julia + Jupyter + GPU = âš—ï¸ğŸ”¬ğŸ§¬ğŸ¥°\u001b[22m\n",
       "    \u001b[2mâ–¡ Outline\u001b[22m\n",
       "    \u001b[2mâ–¡ Motivation\u001b[22m\n",
       "    \u001b[2mâ–¡ Install\u001b[22m\n",
       "    \u001b[2mâ–¡ Basic usage\u001b[22m\n",
       "    \u001b[2mâ–¡ Power of Julia (1)\u001b[22m\n",
       "    \u001b[2mâ–¡ Limitations\u001b[22m\n",
       "    \u001b[2mâ–¡ Power of Julia (2)\u001b[22m\n",
       "    \u001b[2mâ–¡ Multi-GPU (single node)\u001b[22m\n",
       "    \u001b[2mâ–¡ Multi-GPU (multiple nodes, elastic)\u001b[22m\n",
       "    \u001b[2mâ–¡ Multi-GPU (multiple nodes, MPI)\u001b[22m\n",
       "    \u001b[2mâ–¡ Multi-GPU (multiple nodes, MPI, notebooks)\u001b[22m\n",
       "      \u001b[1m\u001b[32mâ˜’ \u001b[39mSome code in a notebook:\u001b[22m\n",
       "        \u001b[1m\u001b[32mâ˜’ \u001b[39m\u001b[22m\u001b[1mâ€¦\u001b[22m\n",
       "      \u001b[2mâ–¡ Now use:\u001b[22m\n",
       "    \u001b[2mâ–¡ Conclusions\u001b[22m"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = ParameterizedNotebook(\"talk.ipynb\", sections=(\"Some code in a notebook:\",))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1f025ce3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  85.074 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 5:\t  82.029 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 6:\t  82.058 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 7:\t  81.968 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 9:\t  81.999 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 3:\t  81.838 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 8:\t  81.818 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 10:\t  81.929 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 4:\t  82.148 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 11:\t  81.808 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 13:\t  82.149 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 12:\t  81.799 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 14:\t  81.948 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 2:\t  82.078 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 16:\t  81.909 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 17:\t  81.908 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 15:\t  81.689 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 18:\t  82.139 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 19:\t  81.988 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 21:\t  81.537 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 20:\t  81.898 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 22:\t  81.698 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 23:\t  81.968 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 25:\t  81.859 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 24:\t  81.508 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 26:\t  81.769 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 27:\t  82.099 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 29:\t  81.728 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 28:\t  81.598 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 30:\t  81.899 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 31:\t  82.019 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 33:\t  81.938 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 32:\t  81.638 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 34:\t  81.758 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 35:\t  81.958 Î¼s (37 allocations: 1.91 KiB)\n",
      "      From worker 36:\t  81.908 Î¼s (37 allocations: 1.91 KiB)\n"
     ]
    }
   ],
   "source": [
    "nb()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc4015d-af2c-4266-8cc0-ec9f3b384f44",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can put the call to the notebook code directly in a `test_script_2.jl`:\n",
    "```julia\n",
    "#!/bin/bash\n",
    "#SBATCH -C gpu -q regular -A mp107\n",
    "#SBATCH -t 00:05:00 \n",
    "#SBATCH --cpus-per-task 32 --gpus-per-task 1 --ntasks-per-node 4 --nodes 4\n",
    "#=\n",
    "srun /global/u1/m/marius/.julia/juliaup/julia-1.9.3+0.x64.linux.gnu/bin/julia $(scontrol show job $SLURM_JOBID | awk -F= '/Command=/{print $2}')\n",
    "exit 0\n",
    "# =#\n",
    "\n",
    "using MPIClusterManagers, Distributed, CUDA\n",
    "mgr = MPIClusterManagers.start_main_loop(MPIClusterManagers.MPI_TRANSPORT_ALL)\n",
    "\n",
    "nb = ParameterizedNotebook(\"talk.ipynb\", sections=(\"Some code in a notebook:\",))\n",
    "nb()\n",
    "\n",
    "MPIClusterManagers.stop_main_loop(mgr)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbe2c3b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "With some care in the organization of your sections, you can iterate on code in the notebook, even test it in parallel using on-the-fly `ElasticManager` workers, then submit the identical code as an MPI job for larger-scale runs ğŸ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3405e4d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec83b8be-279c-47be-822f-d47f9986adfa",
   "metadata": {},
   "source": [
    "* Julia + Jupyter + GPUs offer powerful scientific workflows\n",
    "* Hopefully I've shared some efficient ways to do this that we've learned\n",
    "* Wishlist\n",
    "    * More robust and easier CUDA.jl task/threading support\n",
    "    * An easy way to use MPI CUDA transport protocol from within Jupyter jobs\n",
    "    * A _multi-node_ GPU monitor, even just a command-line one\n",
    "        * `nvitop`, `btop` (PR), and `gpustat` are some good command line single-node options"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
