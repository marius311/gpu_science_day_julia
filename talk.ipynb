{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f3028ecd-a924-40cd-a9fb-919ab9b3f8b4",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".reveal pre code {\n",
       "    max-height: none;\n",
       "    font-size: 90%;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "HTML{String}(\"<style>\\n.reveal pre code {\\n    max-height: none;\\n    font-size: 90%;\\n}\\n</style>\\n\")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENV[\"LINES\"] = 10;\n",
    "ENV[\"COLUMNS\"] = 80;\n",
    "\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".reveal pre code {\n",
    "    max-height: none;\n",
    "    font-size: 90%;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63fb033",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Julia + Jupyter + GPU = ⚗️🔬🧬🥰\n",
    "\n",
    "Marius Millea (Project Scientist @ UC Davis in Cosmology)\n",
    "\n",
    "NERSC GPU Science Day, Oct 12, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a614681",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "* Motivation\n",
    "* Julia CUDA Installation\n",
    "* Basic and advanced Julia CUDA usage\n",
    "* Multi-GPU workflows for embarrasingly parallel problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b6d67e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdb55c5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Julia\n",
    "    * interactive but fast\n",
    "    * powerful and flexible\n",
    "    * less boilerplate: code looks like science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6228a33e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Jupyter\n",
    "    * convenient for interactive work\n",
    "    * fast iterative development workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97cfc77",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* GPU\n",
    "    * duh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea42a49-3f34-4061-83f3-d55b3f9edbdc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bfb764",
   "metadata": {},
   "source": [
    "Julia/CUDA install is drop-dead simple. Julia's CUDA package provides compatible binary drivers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1600af3",
   "metadata": {},
   "source": [
    "```shell\n",
    "$ curl -fsSL https://install.julialang.org | sh\n",
    "$ julia\n",
    "pkg> add CUDA # ~2min\n",
    "   Resolving package versions...\n",
    "   Installed CUDA_Driver_jll ── v0.6.0+3\n",
    "   Installed LLVMExtra_jll ──── v0.0.26+0\n",
    "   ...\n",
    "   Installed CUDA ───────────── v5.0.0\n",
    " Downloading artifact: CUDA_Driver\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1318c9b9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "(Easy to select CUDA version _per project_ with e.g. `CUDA.set_runtime_version!(v\"11.4\")`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf466c7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "I recommend this fully native Julia install over using any `modules`, i.e. I don't even have the `gpu` module loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "349bac64-41a5-488a-888c-b9305be35e28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Currently Loaded Modules:\n",
      "  1) craype-x86-milan                        8) cray-mpich/8.1.25\n",
      "  2) libfabric/1.15.2.0                      9) craype/2.7.20\n",
      "  3) craype-network-ofi                     10) gcc/11.2.0\n",
      "  4) xpmem/2.6.2-2.5_2.27__gd067c3f.shasta  11) perftools-base/23.03.0\n",
      "  5) PrgEnv-gnu/8.3.3                       12) cpe/23.03\n",
      "  6) cray-dsmml/0.2.2                       13) xalt/2.10.2\n",
      "  7) cray-libsci/23.02.1.1                  14) cray-python/3.9.13.1   (dev)\n",
      "\n",
      "  Where:\n",
      "   dev:  Development Tools and Programming Languages\n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "; module list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108b56a5",
   "metadata": {},
   "source": [
    "This has proven robust across many clusters I've tried."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d35ade1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Checking everything is installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02f49430-09f8-460a-a439-4cf0c3f93b8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e3bb8bd-d25f-4c8b-9b2d-5f13c681bc75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA runtime 12.2, artifact installation\n",
      "CUDA driver 12.2\n",
      "NVIDIA driver 525.105.17, originally for CUDA 12.0\n",
      "\n",
      "CUDA libraries: \n",
      "- CUBLAS: 12.2.5\n",
      "- CURAND: 10.3.3\n",
      "- CUFFT: 11.0.8\n",
      "- CUSOLVER: 11.5.2\n",
      "- CUSPARSE: 12.1.2\n",
      "- CUPTI: 20.0.0\n",
      "- NVML: 12.0.0+525.105.17\n",
      "\n",
      "Julia packages: \n",
      "- CUDA: 5.0.0\n",
      "- CUDA_Driver_jll: 0.6.0+3\n",
      "- CUDA_Runtime_jll: 0.9.2+0\n",
      "\n",
      "Toolchain:\n",
      "- Julia: 1.9.3\n",
      "- LLVM: 14.0.6\n",
      "- PTX ISA support: 3.2, 4.0, 4.1, 4.2, 4.3, 5.0, 6.0, 6.1, 6.3, 6.4, 6.5, 7.0, 7.1, 7.2, 7.3, 7.4, 7.5\n",
      "- Device capability support: sm_37, sm_50, sm_52, sm_53, sm_60, sm_61, sm_62, sm_70, sm_72, sm_75, sm_80, sm_86\n",
      "\n",
      "4 devices:\n",
      "  0: NVIDIA A100-SXM4-40GB (sm_80, 39.389 GiB / 40.000 GiB available)\n",
      "  1: NVIDIA A100-SXM4-40GB (sm_80, 39.389 GiB / 40.000 GiB available)\n",
      "  2: NVIDIA A100-SXM4-40GB (sm_80, 39.389 GiB / 40.000 GiB available)\n",
      "  3: NVIDIA A100-SXM4-40GB (sm_80, 39.389 GiB / 40.000 GiB available)\n"
     ]
    }
   ],
   "source": [
    "CUDA.versioninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5734821-7675-41c5-a547-a02157039f4c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c90f66cf-19ac-4d6d-b4b1-0e6e4e3febd9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000-element Vector{Float64}:\n",
       " 0.9086257270453094\n",
       " 0.09199119500242203\n",
       " 0.5405112272332471\n",
       " ⋮\n",
       " 0.1508900262074172\n",
       " 0.9024278712197097"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = rand(10_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e854be87-f7ec-4206-8410-698b1242cf33",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 0.9086257\n",
       " 0.09199119\n",
       " 0.54051125\n",
       " ⋮\n",
       " 0.15089002\n",
       " 0.90242785"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carr = cu(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "470104fb-7a46-4a34-b6e8-f32e9230b930",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 1.7886596\n",
       " 1.0918615\n",
       " 1.5145744\n",
       " ⋮\n",
       " 1.1503181\n",
       " 1.7848338"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sin.(carr) .+ 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009c3528",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lets benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4e7e49f-a8d9-4656-8e71-c03aa4f95fc0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "using BenchmarkTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d91b6b2-e853-4536-8109-1eec8e4ab8da",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  83.481 μs (41 allocations: 2.00 KiB)\n"
     ]
    }
   ],
   "source": [
    "@btime CUDA.@sync sin.(carr) .+ 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "321dcf26-1d65-49c5-8c25-79df78c8d97d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  68.553 ms (6 allocations: 76.29 MiB)\n"
     ]
    }
   ],
   "source": [
    "@btime sin.(arr) .+ 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "970e23ed-a0ef-401e-a22f-df98921f7ba8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiler ran for 300.17 µs, capturing 11 events.\n",
      "\n",
      "Host-side activity: calling CUDA APIs took 56.98 µs (18.98% of the trace)\n",
      "┌──────────┬──────────┬───────┬──────────┬──────────┬──────────┬─────────────────────────┐\n",
      "│\u001b[1m Time (%) \u001b[0m│\u001b[1m     Time \u001b[0m│\u001b[1m Calls \u001b[0m│\u001b[1m Avg time \u001b[0m│\u001b[1m Min time \u001b[0m│\u001b[1m Max time \u001b[0m│\u001b[1m Name                    \u001b[0m│\n",
      "├──────────┼──────────┼───────┼──────────┼──────────┼──────────┼─────────────────────────┤\n",
      "│    9.69% │\u001b[31m 29.09 µs \u001b[0m│     1 │ 29.09 µs │ 29.09 µs │ 29.09 µs │\u001b[1m cuLaunchKernel          \u001b[0m│\n",
      "│    7.39% │ 22.17 µs │     1 │ 22.17 µs │ 22.17 µs │ 22.17 µs │ cuMemAllocFromPoolAsync │\n",
      "└──────────┴──────────┴───────┴──────────┴──────────┴──────────┴─────────────────────────┘\n",
      "\n",
      "Device-side activity: GPU was busy for 80.59 µs (26.85% of the trace)\n",
      "┌──────────┬──────────┬───────┬──────────┬──────────┬──────────┬───────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "│\u001b[1m Time (%) \u001b[0m│\u001b[1m     Time \u001b[0m│\u001b[1m Calls \u001b[0m│\u001b[1m Avg time \u001b[0m│\u001b[1m Min time \u001b[0m│\u001b[1m Max time \u001b[0m│\u001b[1m Name                                                                                                        \u001b[0m ⋯\n",
      "├──────────┼──────────┼───────┼──────────┼──────────┼──────────┼───────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "│   26.85% │\u001b[31m 80.59 µs \u001b[0m│     1 │ 80.59 µs │ 80.59 µs │ 80.59 µs │\u001b[1m _Z16broadcast_kernel15CuKernelContext13CuDeviceArrayI7Float32Li1ELi1EE11BroadcastedI12CuArrayStyleILi1EE5Tup\u001b[0m ⋯\n",
      "└──────────┴──────────┴───────┴──────────┴──────────┴──────────┴───────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\u001b[36m                                                                                                                                                               1 column omitted\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "CUDA.@profile sin.(carr) .+ 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4506ce3c-0868-4eea-bbc7-b3004d589e60",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Power of Julia (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d5e6c0",
   "metadata": {},
   "source": [
    "In Julia, you can easily put many arbitrary objects on GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "382f1b13-28ee-4a9d-b38a-bec483b0254e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "struct Point{T}\n",
    "    x :: T\n",
    "    y :: T\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "35ae9488-9d0b-4c43-8012-5b476c2bdfed",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100-element CuArray{Point{Float64}, 1, CUDA.Mem.DeviceBuffer}:\n",
       " Point{Float64}(0.9036834915690581, 0.7280460429572848)\n",
       " Point{Float64}(0.7708203673107145, 0.05294668526958213)\n",
       " Point{Float64}(0.5394529633521908, 0.8032629228332351)\n",
       " ⋮\n",
       " Point{Float64}(0.6624703153358394, 0.4720296666892744)\n",
       " Point{Float64}(0.8116771423344411, 0.8097130787459793)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = Point.(rand(100), rand(100))\n",
    "carr = cu(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9555b43",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In e.g. Jax/PyTorch/TF, the only things you can stick inside of CUDA arrays are Int/Float/Complex. In Julia, anything with a static memory layout is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "528d1f95-c491-4e96-94cb-e0c976c511d6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "distance_from_origin (generic function with 1 method)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_from_origin(p::Point) = sqrt(p.x^2 + p.y^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ba152df7-0d00-477c-9eb1-ddfe355b4cfe",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100-element CuArray{Float64, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 1.1604718409337662\n",
       " 0.7726366482002138\n",
       " 0.9675953817934705\n",
       " ⋮\n",
       " 0.8134364910280052\n",
       " 1.1464968614350834"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_from_origin.(carr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbac985e-bc4e-4579-a709-ab0b37399fea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "832d529f-7dfd-48f4-a5ae-eb80b08c3fa9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "distance_from_origin_bad (generic function with 1 method)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function distance_from_origin_bad(p::Point)\n",
    "    sqrt(sum([p.x^2, p.y^2]))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "727c6377-d9d2-42e2-a8b4-45cef1ed4db8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "InvalidIRError: compiling MethodInstance for (::GPUArrays.var\"#broadcast_kernel#32\")(::CUDA.CuKernelContext, ::CuDeviceVector{Float64, 1}, ::Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{1}, Tuple{Base.OneTo{Int64}}, typeof(distance_from_origin_bad), Tuple{Base.Broadcast.Extruded{CuDeviceVector{Point{Float64}, 1}, Tuple{Bool}, Tuple{Int64}}}}, ::Int64) resulted in invalid LLVM IR\n\u001b[31mReason: unsupported call through a literal pointer\u001b[39m\u001b[31m (call to ijl_alloc_array_1d)\u001b[39m\nStacktrace:\n  [1] \u001b[0m\u001b[1mArray\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mboot.jl:477\u001b[24m\u001b[39m\n  [2] \u001b[0m\u001b[1mArray\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mboot.jl:486\u001b[24m\u001b[39m\n  [3] \u001b[0m\u001b[1msimilar\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mabstractarray.jl:884\u001b[24m\u001b[39m\n  [4] \u001b[0m\u001b[1msimilar\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mabstractarray.jl:883\u001b[24m\u001b[39m\n  [5] \u001b[0m\u001b[1m_array_for\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4marray.jl:671\u001b[24m\u001b[39m\n  [6] \u001b[0m\u001b[1m_array_for\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4marray.jl:674\u001b[24m\u001b[39m\n  [7] \u001b[0m\u001b[1mvect\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4marray.jl:126\u001b[24m\u001b[39m\n  [8] \u001b[0m\u001b[1mdistance_from_origin_bad\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mIn[20]:2\u001b[24m\u001b[39m\n  [9] \u001b[0m\u001b[1m_broadcast_getindex_evalf\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mbroadcast.jl:683\u001b[24m\u001b[39m\n [10] \u001b[0m\u001b[1m_broadcast_getindex\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mbroadcast.jl:656\u001b[24m\u001b[39m\n [11] \u001b[0m\u001b[1mgetindex\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mbroadcast.jl:610\u001b[24m\u001b[39m\n [12] \u001b[0m\u001b[1mbroadcast_kernel\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m~/.julia/packages/GPUArrays/EZkix/src/host/\u001b[39m\u001b[90m\u001b[4mbroadcast.jl:64\u001b[24m\u001b[39m\n\u001b[36m\u001b[1mHint\u001b[22m\u001b[39m\u001b[36m: catch this exception as `err` and call `code_typed(err; interactive = true)` to introspect the erronous code with Cthulhu.jl\u001b[39m",
     "output_type": "error",
     "traceback": [
      "InvalidIRError: compiling MethodInstance for (::GPUArrays.var\"#broadcast_kernel#32\")(::CUDA.CuKernelContext, ::CuDeviceVector{Float64, 1}, ::Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{1}, Tuple{Base.OneTo{Int64}}, typeof(distance_from_origin_bad), Tuple{Base.Broadcast.Extruded{CuDeviceVector{Point{Float64}, 1}, Tuple{Bool}, Tuple{Int64}}}}, ::Int64) resulted in invalid LLVM IR\n\u001b[31mReason: unsupported call through a literal pointer\u001b[39m\u001b[31m (call to ijl_alloc_array_1d)\u001b[39m\nStacktrace:\n  [1] \u001b[0m\u001b[1mArray\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mboot.jl:477\u001b[24m\u001b[39m\n  [2] \u001b[0m\u001b[1mArray\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mboot.jl:486\u001b[24m\u001b[39m\n  [3] \u001b[0m\u001b[1msimilar\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mabstractarray.jl:884\u001b[24m\u001b[39m\n  [4] \u001b[0m\u001b[1msimilar\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mabstractarray.jl:883\u001b[24m\u001b[39m\n  [5] \u001b[0m\u001b[1m_array_for\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4marray.jl:671\u001b[24m\u001b[39m\n  [6] \u001b[0m\u001b[1m_array_for\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4marray.jl:674\u001b[24m\u001b[39m\n  [7] \u001b[0m\u001b[1mvect\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4marray.jl:126\u001b[24m\u001b[39m\n  [8] \u001b[0m\u001b[1mdistance_from_origin_bad\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mIn[20]:2\u001b[24m\u001b[39m\n  [9] \u001b[0m\u001b[1m_broadcast_getindex_evalf\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mbroadcast.jl:683\u001b[24m\u001b[39m\n [10] \u001b[0m\u001b[1m_broadcast_getindex\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mbroadcast.jl:656\u001b[24m\u001b[39m\n [11] \u001b[0m\u001b[1mgetindex\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mbroadcast.jl:610\u001b[24m\u001b[39m\n [12] \u001b[0m\u001b[1mbroadcast_kernel\u001b[22m\n\u001b[90m    @\u001b[39m \u001b[90m~/.julia/packages/GPUArrays/EZkix/src/host/\u001b[39m\u001b[90m\u001b[4mbroadcast.jl:64\u001b[24m\u001b[39m\n\u001b[36m\u001b[1mHint\u001b[22m\u001b[39m\u001b[36m: catch this exception as `err` and call `code_typed(err; interactive = true)` to introspect the erronous code with Cthulhu.jl\u001b[39m",
      "",
      "Stacktrace:",
      "  [1] check_ir(job::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams}, args::LLVM.Module)",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/2mJjc/src/validation.jl:147",
      "  [2] macro expansion",
      "    @ ~/.julia/packages/GPUCompiler/2mJjc/src/driver.jl:440 [inlined]",
      "  [3] macro expansion",
      "    @ ~/.julia/packages/TimerOutputs/RsWnF/src/TimerOutput.jl:253 [inlined]",
      "  [4] macro expansion",
      "    @ ~/.julia/packages/GPUCompiler/2mJjc/src/driver.jl:439 [inlined]",
      "  [5] emit_llvm(job::GPUCompiler.CompilerJob; libraries::Bool, toplevel::Bool, optimize::Bool, cleanup::Bool, only_entry::Bool, validate::Bool)",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/2mJjc/src/utils.jl:92",
      "  [6] codegen(output::Symbol, job::GPUCompiler.CompilerJob; libraries::Bool, toplevel::Bool, optimize::Bool, cleanup::Bool, strip::Bool, validate::Bool, only_entry::Bool, parent_job::Nothing)",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/2mJjc/src/driver.jl:129",
      "  [7] codegen",
      "    @ ~/.julia/packages/GPUCompiler/2mJjc/src/driver.jl:110 [inlined]",
      "  [8] compile(target::Symbol, job::GPUCompiler.CompilerJob; libraries::Bool, toplevel::Bool, optimize::Bool, cleanup::Bool, strip::Bool, validate::Bool, only_entry::Bool)",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/2mJjc/src/driver.jl:106",
      "  [9] compile",
      "    @ ~/.julia/packages/GPUCompiler/2mJjc/src/driver.jl:98 [inlined]",
      " [10] #1042",
      "    @ ~/.julia/packages/CUDA/nbRJk/src/compiler/compilation.jl:166 [inlined]",
      " [11] JuliaContext(f::CUDA.var\"#1042#1045\"{GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams}})",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/2mJjc/src/driver.jl:47",
      " [12] compile(job::GPUCompiler.CompilerJob)",
      "    @ CUDA ~/.julia/packages/CUDA/nbRJk/src/compiler/compilation.jl:165",
      " [13] actual_compilation(cache::Dict{Any, CuFunction}, src::Core.MethodInstance, world::UInt64, cfg::GPUCompiler.CompilerConfig{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams}, compiler::typeof(CUDA.compile), linker::typeof(CUDA.link))",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/2mJjc/src/execution.jl:125",
      " [14] cached_compilation(cache::Dict{Any, CuFunction}, src::Core.MethodInstance, cfg::GPUCompiler.CompilerConfig{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams}, compiler::Function, linker::Function)",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/2mJjc/src/execution.jl:103",
      " [15] macro expansion",
      "    @ ~/.julia/packages/CUDA/nbRJk/src/compiler/execution.jl:323 [inlined]",
      " [16] macro expansion",
      "    @ ./lock.jl:267 [inlined]",
      " [17] cufunction(f::GPUArrays.var\"#broadcast_kernel#32\", tt::Type{Tuple{CUDA.CuKernelContext, CuDeviceVector{Float64, 1}, Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{1}, Tuple{Base.OneTo{Int64}}, typeof(distance_from_origin_bad), Tuple{Base.Broadcast.Extruded{CuDeviceVector{Point{Float64}, 1}, Tuple{Bool}, Tuple{Int64}}}}, Int64}}; kwargs::Base.Pairs{Symbol, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})",
      "    @ CUDA ~/.julia/packages/CUDA/nbRJk/src/compiler/execution.jl:318",
      " [18] cufunction",
      "    @ ~/.julia/packages/CUDA/nbRJk/src/compiler/execution.jl:315 [inlined]",
      " [19] macro expansion",
      "    @ ~/.julia/packages/CUDA/nbRJk/src/compiler/execution.jl:104 [inlined]",
      " [20] #launch_heuristic#1087",
      "    @ ~/.julia/packages/CUDA/nbRJk/src/gpuarrays.jl:17 [inlined]",
      " [21] launch_heuristic",
      "    @ ~/.julia/packages/CUDA/nbRJk/src/gpuarrays.jl:15 [inlined]",
      " [22] _copyto!",
      "    @ ~/.julia/packages/GPUArrays/EZkix/src/host/broadcast.jl:70 [inlined]",
      " [23] copyto!",
      "    @ ~/.julia/packages/GPUArrays/EZkix/src/host/broadcast.jl:51 [inlined]",
      " [24] copy",
      "    @ ~/.julia/packages/GPUArrays/EZkix/src/host/broadcast.jl:42 [inlined]",
      " [25] materialize(bc::Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{1}, Nothing, typeof(distance_from_origin_bad), Tuple{CuArray{Point{Float64}, 1, CUDA.Mem.DeviceBuffer}}})",
      "    @ Base.Broadcast ./broadcast.jl:873",
      " [26] top-level scope",
      "    @ In[21]:1"
     ]
    }
   ],
   "source": [
    "distance_from_origin_bad.(carr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807107d4-73c8-473a-9d13-737c2311897c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Limitations on code in functions that will be compiled for GPU:\n",
    "* No calls to CPU functions\n",
    "   * E.g. creating Arrays (use StaticArrays.jl instead)\n",
    "* No _dynamic dispatch_\n",
    "   * Code should be _type stable_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cae2fe-6444-479e-a1f6-98acb5800ba5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Power of Julia (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf0865",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You can also directly write kernels in Julia, giving you the full power and flexibility of CUDA kernel programming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1ad299e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "my_kernel (generic function with 1 method)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function my_kernel(carr_out, carr)   \n",
    "    start = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n",
    "    stride = blockDim().x * gridDim().x\n",
    "    len = length(carr)\n",
    "    for i = start:stride:len  # \"grid-stride\" loop\n",
    "        carr_out[i] = sin(carr[i]) + 1\n",
    "    end\n",
    "    return\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "635c6648",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "carr = cu(rand(10_000_000))\n",
    "carr_out = similar(carr);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ab21fce",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CUDA.HostKernel for my_kernel(CuDeviceVector{Float32, 1}, CuDeviceVector{Float32, 1})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@cuda threads=256 my_kernel(carr_out, carr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce0793f0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 1.1289185\n",
       " 1.385206\n",
       " 1.44538\n",
       " ⋮\n",
       " 1.8264189\n",
       " 1.7915335"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carr_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0c5d3c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "See [Kernel Programming](https://cuda.juliagpu.org/stable/api/kernel/) for full list of CUDA.jl kernel programming capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ae7b4e-0547-4f11-83aa-94433a2af38d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-GPU (single node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2ead7fe-334b-4c1a-a6d6-909dd01d7ff5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CUDA.DeviceIterator() for 4 devices:\n",
       "0. NVIDIA A100-SXM4-40GB\n",
       "1. NVIDIA A100-SXM4-40GB\n",
       "2. NVIDIA A100-SXM4-40GB\n",
       "3. NVIDIA A100-SXM4-40GB"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUDA.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc5b554c-33e9-4512-b0e3-41014eb3106b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CuDevice(0): NVIDIA A100-SXM4-40GB"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUDA.device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e3976cb-3305-44ac-97b7-3dd7363646e7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CuDevice(1): NVIDIA A100-SXM4-40GB"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUDA.device!(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65eaa3e0-3097-44df-8402-c43a20c88d8e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  84.943 μs (41 allocations: 2.00 KiB)\n"
     ]
    }
   ],
   "source": [
    "arr = rand(10_000_000)\n",
    "carr = cu(arr)\n",
    "@btime CUDA.@sync sin.(carr) .+ 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbee4a7-f1fc-43c3-a2ac-95e40758bc07",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "CUDA.jl does its own memory management, so before switching back to GPU 0, give back memory (don't usually have to think about this unless you use the same GPU from multiple processes, which for the purpose of this demo I do):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0da831ed-5f8e-4886-b7d6-1fbc7acff2eb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "GC.gc()\n",
    "CUDA.reclaim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a29fbf8c-f564-4d65-a4d9-b73321fa11c7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CuDevice(0): NVIDIA A100-SXM4-40GB"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUDA.device!(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e376af19",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can use multiple GPUs via Julia processes, tasks, or threads. \n",
    "\n",
    "The most robust and easy way I have found (as of 2023), which I recommend starting with, is per-_process_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c2f51e76-6517-4a49-95e2-5a3459d63a81",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "using Distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d940d90-6514-4d8d-9e3b-07da5a60d32f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Int64}:\n",
       " 2\n",
       " 3\n",
       " 4"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addprocs(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "49108a26-d4fb-4fa4-905c-a71a71d8f5a2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@everywhere using CUDA, BenchmarkTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4094404e-7f67-488c-ba27-71270b8284b2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, CuDevice(0))\n",
      "      From worker 2:\t(2, CuDevice(0))\n",
      "      From worker 4:\t(4, CuDevice(0))\n",
      "      From worker 3:\t(3, CuDevice(0))\n"
     ]
    }
   ],
   "source": [
    "@everywhere procs() println((myid(), CUDA.device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9202be9-166c-4416-abf6-57b9bdd9da47",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@everywhere procs() CUDA.device!(myid()-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "77aa9ba8-faaf-42bd-8c58-86c4423a468b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, CuDevice(0))\n",
      "      From worker 4:\t(4, CuDevice(3))\n",
      "      From worker 3:\t(3, CuDevice(2))\n",
      "      From worker 2:\t(2, CuDevice(1))\n"
     ]
    }
   ],
   "source": [
    "@everywhere procs() println((myid(), CUDA.device()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119909c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lets run our benchmark in parallel across all GPUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7edbb80-4e02-462c-b2ca-127c35b14130",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  84.954 μs (37 allocations: 1.91 KiB)\n",
      "      From worker 4:\t  81.557 μs (37 allocations: 1.91 KiB)\n",
      "      From worker 3:\t  81.547 μs (37 allocations: 1.91 KiB)\n",
      "      From worker 2:\t  81.196 μs (37 allocations: 1.91 KiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4-element Vector{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}:\n",
       " Float32[1.730833, 1.1198826, 1.3534169, 1.7853978, 1.5614758, 1.4536244, 1.4328939, 1.367788, 1.5307729, 1.6284117  …  1.2086698, 1.7418078, 1.5088024, 1.6645186, 1.3867525, 1.2837766, 1.7777662, 1.512737, 1.6256388, 1.442714]\n",
       " Float32[1.730833, 1.1198826, 1.3534169, 1.7853978, 1.5614758, 1.4536244, 1.4328939, 1.367788, 1.5307729, 1.6284117  …  1.2086698, 1.7418078, 1.5088024, 1.6645186, 1.3867525, 1.2837766, 1.7777662, 1.512737, 1.6256388, 1.442714]\n",
       " Float32[1.730833, 1.1198826, 1.3534169, 1.7853978, 1.5614758, 1.4536244, 1.4328939, 1.367788, 1.5307729, 1.6284117  …  1.2086698, 1.7418078, 1.5088024, 1.6645186, 1.3867525, 1.2837766, 1.7777662, 1.512737, 1.6256388, 1.442714]\n",
       " Float32[1.730833, 1.1198826, 1.3534169, 1.7853978, 1.5614758, 1.4536244, 1.4328939, 1.367788, 1.5307729, 1.6284117  …  1.2086698, 1.7418078, 1.5088024, 1.6645186, 1.3867525, 1.2837766, 1.7777662, 1.512737, 1.6256388, 1.442714]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let\n",
    "    carr = cu(rand(10_000_000))\n",
    "    pmap(WorkerPool(procs()), 1:4) do i\n",
    "        @btime CUDA.@sync sin.($carr) .+ 1\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7edc333-7c3a-4760-95e0-b7e10f2789c8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note, `carr` was defined and moved to GPU on the master process. Julia automatically sent it to the worker GPUs, then automatically sent the results back to the master GPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1c3999-845c-4b3e-8e54-a2215aa3801b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In doing so, the array passed through CPU memory, so its not the most efficient (but its the easiest)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fafedca-7e29-4c4d-86a8-9edcca2e4718",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To go straight GPU-to-GPU, you can use _unified memory_ on a single-node, or CUDA MPI transport (later this talk)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bfa320-711a-487f-92ef-526728c517c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-GPU (multiple nodes, elastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cbe338f9-afc9-4648-9b27-f59c1c21e543",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "using ClusterManagers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d8bf1b4a-344f-4d43-b485-ae04335339cc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "em = ElasticManager(\n",
    "    # Perlmutter specific ↓\n",
    "    addr = IPv4(first(filter(!isnothing, match.(r\"inet (.*)/.*hsn0\", readlines(`ip a show`)))).captures[1]),\n",
    "    port = 0\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4f2284ef-47ab-40fa-8e11-28f2c4bdefba",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticManager:\n",
       "  Active workers : [ 5,6,7,8,9,10,11,12]\n",
       "  Number of workers to be added  : 0\n",
       "  Terminated workers : []\n",
       "  Worker connect command : \n",
       "    /global/u1/m/marius/.julia/juliaup/julia-1.9.3+0.x64.linux.gnu/bin/julia --project=/global/u1/m/marius/work/gpu_science_day_julia/Project.toml -e 'using ClusterManagers; ClusterManagers.elastic_worker(\"JUYPFO3Hwb8flWSH\",\"10.249.6.77\",39653)'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44088681-107f-4268-8b5b-d1e7c173fd98",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now submit a job, e.g. with:\n",
    "```bash\n",
    "salloc -C gpu -q regular -t 00:30:00 --cpus-per-task 32  --gpus-per-task 1 --ntasks-per-node 4 --nodes 8 -A mp107\n",
    "```\n",
    "then run the \"worker connect command\" printed above (could also do all-in-one as a batch job)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a176755",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "With more GPUs across different nodes, its more complex to assign one unique GPU to each process. Instead we can use this utility function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "582fa7c0-1f84-46b7-852e-5afc9df059ee",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "using CUDADistributedTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a6d35638-7a08-4b9f-88ae-787a54b4b781",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mProcesses (4):\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m (myid = 1, host = nid001293, device = CuDevice(0): NVIDIA A100-SXM4-40GB 1c40175b))\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m (myid = 2, host = nid001293, device = CuDevice(1): NVIDIA A100-SXM4-40GB f179efe2))\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m (myid = 3, host = nid001293, device = CuDevice(2): NVIDIA A100-SXM4-40GB 36d32866))\n",
      "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m (myid = 4, host = nid001293, device = CuDevice(3): NVIDIA A100-SXM4-40GB 634451b9))\n"
     ]
    }
   ],
   "source": [
    "CUDADistributedTools.assign_GPU_workers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1143ee65",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's run parallel benchmarks again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6c79cdcd-2c28-4c03-aa33-9aac7c239fbe",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@everywhere using CUDA, BenchmarkTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "199904a6-ccb2-4bcb-b358-b9a9830fdc6a",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  85.044 μs (37 allocations: 1.91 KiB)\n",
      "      From worker 2:\t  81.798 μs (37 allocations: 1.91 KiB)\n",
      "      From worker 3:\t  81.747 μs (37 allocations: 1.91 KiB)\n",
      "      From worker 4:\t  82.018 μs (37 allocations: 1.91 KiB)\n"
     ]
    }
   ],
   "source": [
    "let\n",
    "    carr = cu(rand(10_000_000))\n",
    "    pmap(WorkerPool(procs()), 1:nprocs()) do i\n",
    "        @btime CUDA.@sync sin.($carr) .+ 1\n",
    "        return nothing\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e7a87f-4286-4a7e-a087-304ed97b32da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-GPU (multiple nodes, MPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cbf52d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Installing MPI for Julia and configuring:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1b7b25",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "```julia\n",
    "pkg> add MPI MPIPreferences\n",
    "\n",
    "julia> MPIPreferences.use_system_binary(;vendor=\"cray\", mpiexec=\"srun\") # <- options are Perlmutter specific\n",
    "\n",
    "┌ Info: MPI implementation identified\n",
    "│   libmpi = \"libmpi_gnu_91.so\"\n",
    "│   version_string = \"MPI VERSION    : CRAY MPICH version 8.1.25.17 (ANL base 3.4a2)\\nMPI BUILD INFO : Sun Feb 26 15:15 2023 (git hash aecd99f)\\n\"\n",
    "│   impl = \"CrayMPICH\"\n",
    "│   version = v\"8.1.25\"\n",
    "└   abi = \"MPICH\"\n",
    "┌ Info: MPIPreferences changed\n",
    "│   binary = \"system\"\n",
    "│   libmpi = \"libmpi_gnu_91.so\"\n",
    "│   abi = \"MPICH\"\n",
    "│   mpiexec = \"srun\"\n",
    "│   preloads =\n",
    "│    1-element Vector{String}:\n",
    "│     \"libmpi_gtl_cuda.so\"\n",
    "└   preloads_env_switch = \"MPICH_GPU_SUPPORT_ENABLED\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5520ab5e-f687-41a3-bc12-2293241040c2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "(This works thanks to among others NERSC's Johannes Blaschke's contributions to MPI.jl) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7f74b9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "You can put SLURM script and Julia script in one file \n",
    "`test_script.jl`:\n",
    "\n",
    "```julia\n",
    "#!/bin/bash\n",
    "#SBATCH -C gpu -q regular -A mp107\n",
    "#SBATCH -t 00:05:00 \n",
    "#SBATCH --cpus-per-task 32 --gpus-per-task 1 --ntasks-per-node 4 --nodes 4\n",
    "#=\n",
    "srun /global/u1/m/marius/.julia/juliaup/julia-1.9.3+0.x64.linux.gnu/bin/julia $(scontrol show job $SLURM_JOBID | awk -F= '/Command=/{print $2}')\n",
    "exit 0\n",
    "# =#\n",
    "\n",
    "using MPIClusterManagers, Distributed, CUDA\n",
    "mgr = MPIClusterManagers.start_main_loop(MPIClusterManagers.MPI_TRANSPORT_ALL)\n",
    "\n",
    "let\n",
    "    carr = cu(rand(10_000_000))\n",
    "    pmap(WorkerPool(procs()), 1:nprocs()) do i\n",
    "        @btime CUDA.@sync sin.($carr) .+ 1\n",
    "    end\n",
    "end\n",
    "\n",
    "MPIClusterManagers.stop_main_loop(mgr)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8a9574",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Then `sbatch test_script.jl`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9775252-0ffd-47b9-9d3b-f161257bd22b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Here, movement of memory between GPUs will happen via CUDA MPI transport 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da0ab8d-683f-4af1-8df5-2950d74550af",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-GPU (multiple nodes, MPI, notebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1bb015-b678-49e3-82f8-ec8a86e42c07",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Some code in a notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e99065e-7209-4338-8f38-a1a389b5e238",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "let\n",
    "    carr = cu(rand(10_000_000))\n",
    "    pmap(WorkerPool(procs()), 1:nprocs()) do i\n",
    "        @btime CUDA.@sync sin.($carr) .+ 1\n",
    "        return nothing\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6e9c41-451f-4c4f-928b-79fc7c6b5023",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Now use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f29d5e1-5b56-4117-bcc6-76d90a962322",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "using ParameterizedNotebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c600c2-747b-4c6d-ab03-063fada5dde9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nb = ParameterizedNotebook(\"talk.ipynb\", sections=(\"Some code in a notebook:\",))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f025ce3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nb()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc4015d-af2c-4266-8cc0-ec9f3b384f44",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can put the call to the notebook code directly in a `test_script_2.jl`:\n",
    "```julia\n",
    "#!/bin/bash\n",
    "#SBATCH -C gpu -q regular -A mp107\n",
    "#SBATCH -t 00:05:00 \n",
    "#SBATCH --cpus-per-task 32 --gpus-per-task 1 --ntasks-per-node 4 --nodes 4\n",
    "#=\n",
    "srun /global/u1/m/marius/.julia/juliaup/julia-1.9.3+0.x64.linux.gnu/bin/julia $(scontrol show job $SLURM_JOBID | awk -F= '/Command=/{print $2}')\n",
    "exit 0\n",
    "# =#\n",
    "\n",
    "using MPIClusterManagers, Distributed, CUDA\n",
    "mgr = MPIClusterManagers.start_main_loop(MPIClusterManagers.MPI_TRANSPORT_ALL)\n",
    "\n",
    "nb = ParameterizedNotebook(\"talk.ipynb\", sections=(\"Some code in a notebook:\",))\n",
    "nb()\n",
    "\n",
    "MPIClusterManagers.stop_main_loop(mgr)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e1a3b2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "With some care in the organization of your sections, you can iterate on code in the notebook, even test it in parallel using on-the-fly `ElasticManager` workers, then submit the identical code as an MPI job for large-scale runs 🎉"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3405e4d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Wishlist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec83b8be-279c-47be-822f-d47f9986adfa",
   "metadata": {},
   "source": [
    "* More robust and easier CUDA.jl task/threading support\n",
    "* An easy way to use MPI CUDA transport protocol from within Jupyter jobs\n",
    "* A _multi-node_ GPU monitor, even just a command-line one\n",
    "    * `nvitop`, `btop` (PR), and `gpustat` are some good command line single-node options"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
