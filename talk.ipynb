{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3028ecd-a924-40cd-a9fb-919ab9b3f8b4",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".reveal pre code {\n",
       "    max-height: none;\n",
       "    font-size: 90%;\n",
       "}\n",
       ".rise-enabled .text_cell {\n",
       "    font-size: 150%;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "HTML{String}(\"<style>\\n.reveal pre code {\\n    max-height: none;\\n    font-size: 90%;\\n}\\n.rise-enabled .text_cell {\\n    font-size: 150%;\\n}\\n</style>\\n\")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENV[\"LINES\"] = 10;\n",
    "ENV[\"COLUMNS\"] = 80;\n",
    "\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".reveal pre code {\n",
    "    max-height: none;\n",
    "    font-size: 90%;\n",
    "}\n",
    ".rise-enabled .text_cell {\n",
    "    font-size: 150%;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63fb033",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Julia + Jupyter + GPU = ‚öóÔ∏èüî¨üß¨ü•∞\n",
    "\n",
    "Marius Millea (Project Scientist @ UC Davis in Cosmology)\n",
    "\n",
    "NERSC GPU Science Day, Oct 12, 2023\n",
    "\n",
    "Thanks to: Tim Besard + CUDA.jl/Julia contributors, Johannes Blaschke, Rollin Thomas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e1b1ee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "I work on analyzing maps of the Cosmic Microwave Background. Using tiny distortions imprinted by gravitational lensing, we can make maps of where all the dark matter is in the universe. We do so by solving **millions-of-dimensional Bayesian inference problems.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3202e05",
   "metadata": {},
   "source": [
    "<video controls autoplay loop muted width=\"1800\" height=\"600\" source src=\"kappa_forecast.mp4\" type=\"video/mp4\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07fff96",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Our basic code building blocks are array broadcasts and FFTs, which is perfectly suited for GPU. Our group has been using GPUs since the Cori GPU testbed days.\n",
    "\n",
    "But this talk is not about science, but instead **sharing the workflow we've developed over the last ~5 years.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a614681",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "* Julia + Jupyter + GPU motivation\n",
    "* Julia CUDA Installation\n",
    "* Basic and advanced Julia CUDA usage\n",
    "* Multi-GPU workflows for embarrasingly parallel problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b6d67e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdb55c5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Julia\n",
    "    * interactive but fast\n",
    "    * powerful and flexible\n",
    "    * less boilerplate: code looks like science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6228a33e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Jupyter\n",
    "    * convenient for interactive work\n",
    "    * fast iterative development workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97cfc77",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* GPU\n",
    "    * duh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea42a49-3f34-4061-83f3-d55b3f9edbdc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bfb764",
   "metadata": {},
   "source": [
    "Julia/CUDA install is drop-dead simple. Julia's CUDA package provides compatible binary drivers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1600af3",
   "metadata": {},
   "source": [
    "```shell\n",
    "$ curl -fsSL https://install.julialang.org | sh\n",
    "$ julia\n",
    "pkg> add CUDA # ~2min\n",
    "   Resolving package versions...\n",
    "   Installed CUDA_Driver_jll ‚îÄ‚îÄ v0.6.0+3\n",
    "   Installed LLVMExtra_jll ‚îÄ‚îÄ‚îÄ‚îÄ v0.0.26+0\n",
    "   ...\n",
    "   Installed CUDA ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ v5.0.0\n",
    " Downloading artifact: CUDA_Driver\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1318c9b9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "(Easy to select CUDA version _per project_ with e.g. `CUDA.set_runtime_version!(v\"11.4\")`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf466c7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "I recommend this fully native Julia install over using any `modules`, i.e. I don't even have the `gpu` module loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349bac64-41a5-488a-888c-b9305be35e28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "; module list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdcec68",
   "metadata": {},
   "source": [
    "This has proven robust across many clusters I've tried."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d35ade1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Checking everything is installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f49430-09f8-460a-a439-4cf0c3f93b8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3bb8bd-d25f-4c8b-9b2d-5f13c681bc75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.versioninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5734821-7675-41c5-a547-a02157039f4c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90f66cf-19ac-4d6d-b4b1-0e6e4e3febd9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "arr = rand(10_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e854be87-f7ec-4206-8410-698b1242cf33",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "carr = cu(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470104fb-7a46-4a34-b6e8-f32e9230b930",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sin.(carr) .+ 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009c3528",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lets benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e7e49f-a8d9-4656-8e71-c03aa4f95fc0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "using BenchmarkTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d91b6b2-e853-4536-8109-1eec8e4ab8da",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@btime CUDA.@sync sin.(carr) .+ 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321dcf26-1d65-49c5-8c25-79df78c8d97d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@btime sin.(arr) .+ 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970e23ed-a0ef-401e-a22f-df98921f7ba8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.@profile sin.(carr) .+ 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4506ce3c-0868-4eea-bbc7-b3004d589e60",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Power of Julia (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fadaaea",
   "metadata": {},
   "source": [
    "In Julia, you can easily put many arbitrary objects on GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382f1b13-28ee-4a9d-b38a-bec483b0254e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "struct Point{T}\n",
    "    x :: T\n",
    "    y :: T\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ae9488-9d0b-4c43-8012-5b476c2bdfed",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "arr = Point.(rand(100), rand(100))\n",
    "carr = cu(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9555b43",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In e.g. Jax/PyTorch/TF, the only things you can stick inside of CUDA arrays are Int/Float/Complex. In Julia, anything with a static memory layout is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528d1f95-c491-4e96-94cb-e0c976c511d6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "distance_from_origin(p::Point) = sqrt(p.x^2 + p.y^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba152df7-0d00-477c-9eb1-ddfe355b4cfe",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "distance_from_origin.(carr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbac985e-bc4e-4579-a709-ab0b37399fea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832d529f-7dfd-48f4-a5ae-eb80b08c3fa9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "function distance_from_origin_bad(p::Point)\n",
    "    sqrt(sum([p.x^2, p.y^2]))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727c6377-d9d2-42e2-a8b4-45cef1ed4db8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "distance_from_origin_bad.(carr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807107d4-73c8-473a-9d13-737c2311897c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Limitations on code in functions that will be compiled for GPU:\n",
    "* No calls to CPU functions\n",
    "   * E.g. creating Arrays (use StaticArrays.jl instead)\n",
    "* No _dynamic dispatch_\n",
    "   * Code should be _type stable_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cae2fe-6444-479e-a1f6-98acb5800ba5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Power of Julia (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf0865",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You can also directly write kernels in Julia, giving the full power and flexibility of CUDA kernel programming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ad299e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "function my_kernel(carr_out, carr)   \n",
    "    start = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n",
    "    stride = blockDim().x * gridDim().x\n",
    "    len = length(carr)\n",
    "    for i = start:stride:len  # \"grid-stride\" loop\n",
    "        carr_out[i] = sin(carr[i]) + 1\n",
    "    end\n",
    "    return\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635c6648",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "carr = cu(rand(10_000_000))\n",
    "carr_out = similar(carr);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab21fce",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "@cuda threads=256 my_kernel(carr_out, carr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0793f0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "carr_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0c5d3c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "See [Kernel Programming](https://cuda.juliagpu.org/stable/api/kernel/) for full list of CUDA.jl kernel programming capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ae7b4e-0547-4f11-83aa-94433a2af38d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-GPU (single node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ead7fe-334b-4c1a-a6d6-909dd01d7ff5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5b554c-33e9-4512-b0e3-41014eb3106b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3976cb-3305-44ac-97b7-3dd7363646e7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.device!(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eaa3e0-3097-44df-8402-c43a20c88d8e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "arr = rand(10_000_000)\n",
    "carr = cu(arr)\n",
    "@btime CUDA.@sync sin.(carr) .+ 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbee4a7-f1fc-43c3-a2ac-95e40758bc07",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "CUDA.jl does its own memory management, so before switching back to GPU 0, give back memory (don't usually have to think about this unless you use the same GPU from multiple processes, which for the purpose of this demo I do):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da831ed-5f8e-4886-b7d6-1fbc7acff2eb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "GC.gc()\n",
    "CUDA.reclaim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29fbf8c-f564-4d65-a4d9-b73321fa11c7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.device!(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e376af19",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can use multiple GPUs via Julia processes, tasks, or threads. \n",
    "\n",
    "The most robust and easy way I have found (as of 2023), which I recommend starting with, is per-_process_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f51e76-6517-4a49-95e2-5a3459d63a81",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "using Distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d940d90-6514-4d8d-9e3b-07da5a60d32f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "addprocs(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49108a26-d4fb-4fa4-905c-a71a71d8f5a2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@everywhere using CUDA, BenchmarkTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4094404e-7f67-488c-ba27-71270b8284b2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@everywhere procs() println((myid(), CUDA.device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9202be9-166c-4416-abf6-57b9bdd9da47",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@everywhere procs() CUDA.device!(myid()-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aa9ba8-faaf-42bd-8c58-86c4423a468b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@everywhere procs() println((myid(), CUDA.device()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119909c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lets run our benchmark in parallel across all GPUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7edbb80-4e02-462c-b2ca-127c35b14130",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "let\n",
    "    carr = cu(rand(10_000_000))\n",
    "    pmap(WorkerPool(procs()), 1:4) do i\n",
    "        @btime CUDA.@sync sin.($carr) .+ 1\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7edc333-7c3a-4760-95e0-b7e10f2789c8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note, `carr` was defined and moved to GPU on the master process. Julia automatically sent it to the worker GPUs, then automatically sent the results back to the master GPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1c3999-845c-4b3e-8e54-a2215aa3801b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In doing so, the array passed through CPU memory, so its not the most efficient (but its the easiest)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fafedca-7e29-4c4d-86a8-9edcca2e4718",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To go straight GPU-to-GPU, you can use _unified memory_ on a single-node, or CUDA MPI transport (later this talk)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bfa320-711a-487f-92ef-526728c517c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-GPU (multiple nodes, elastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe338f9-afc9-4648-9b27-f59c1c21e543",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "using ClusterManagers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bf1b4a-344f-4d43-b485-ae04335339cc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "em = ElasticManager(\n",
    "    # Perlmutter specific ‚Üì\n",
    "    addr = IPv4(first(filter(!isnothing, match.(r\"inet (.*)/.*hsn0\", readlines(`ip a show`)))).captures[1]),\n",
    "    port = 0\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2284ef-47ab-40fa-8e11-28f2c4bdefba",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "em"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44088681-107f-4268-8b5b-d1e7c173fd98",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now submit a job, e.g. with:\n",
    "```bash\n",
    "salloc -C gpu -q regular -t 00:30:00 --cpus-per-task 32  --gpus-per-task 1 --ntasks-per-node 4 --nodes 8 -A mp107\n",
    "```\n",
    "then run the \"worker connect command\" printed above (could also do all-in-one as a batch job)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ac85b2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "With more GPUs across different nodes, its more complex to assign one unique GPU to each process. Instead we can use this utility function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582fa7c0-1f84-46b7-852e-5afc9df059ee",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "using CUDADistributedTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d35638-7a08-4b9f-88ae-787a54b4b781",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDADistributedTools.assign_GPU_workers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce91d09",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's run parallel benchmarks again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c79cdcd-2c28-4c03-aa33-9aac7c239fbe",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@everywhere using CUDA, BenchmarkTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199904a6-ccb2-4bcb-b358-b9a9830fdc6a",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "let\n",
    "    carr = cu(rand(10_000_000))\n",
    "    pmap(WorkerPool(procs()), 1:nprocs()) do i\n",
    "        @btime CUDA.@sync sin.($carr) .+ 1\n",
    "        return nothing\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e7a87f-4286-4a7e-a087-304ed97b32da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-GPU (multiple nodes, MPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cbf52d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Installing MPI for Julia and configuring:\n",
    "```julia\n",
    "pkg> add MPI MPIPreferences\n",
    "\n",
    "julia> MPIPreferences.use_system_binary(;vendor=\"cray\", mpiexec=\"srun\") # <- options are Perlmutter specific\n",
    "\n",
    "‚îå Info: MPI implementation identified\n",
    "‚îÇ   libmpi = \"libmpi_gnu_91.so\"\n",
    "‚îÇ   version_string = \"MPI VERSION    : CRAY MPICH version 8.1.25.17 (ANL base 3.4a2)\\nMPI BUILD INFO : Sun Feb 26 15:15 2023 (git hash aecd99f)\\n\"\n",
    "‚îÇ   impl = \"CrayMPICH\"\n",
    "‚îÇ   version = v\"8.1.25\"\n",
    "‚îî   abi = \"MPICH\"\n",
    "‚îå Info: MPIPreferences changed\n",
    "‚îÇ   binary = \"system\"\n",
    "‚îÇ   libmpi = \"libmpi_gnu_91.so\"\n",
    "‚îÇ   abi = \"MPICH\"\n",
    "‚îÇ   mpiexec = \"srun\"\n",
    "‚îÇ   preloads =\n",
    "‚îÇ    1-element Vector{String}:\n",
    "‚îÇ     \"libmpi_gtl_cuda.so\"\n",
    "‚îî   preloads_env_switch = \"MPICH_GPU_SUPPORT_ENABLED\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5520ab5e-f687-41a3-bc12-2293241040c2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "(This works thanks to among others NERSC's Johannes Blaschke's contributions to MPI.jl) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f5780f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "You can put SLURM script and Julia script in one file \n",
    "`test_script.jl`:\n",
    "\n",
    "```julia\n",
    "#!/bin/bash\n",
    "#SBATCH -C gpu -q regular -A mp107\n",
    "#SBATCH -t 00:05:00 \n",
    "#SBATCH --cpus-per-task 32 --gpus-per-task 1 --ntasks-per-node 4 --nodes 4\n",
    "#=\n",
    "srun /global/u1/m/marius/.julia/juliaup/julia-1.9.3+0.x64.linux.gnu/bin/julia $(scontrol show job $SLURM_JOBID | awk -F= '/Command=/{print $2}')\n",
    "exit 0\n",
    "# =#\n",
    "\n",
    "using MPIClusterManagers, Distributed, CUDA, BenchmarkTools\n",
    "mgr = MPIClusterManagers.start_main_loop(MPIClusterManagers.MPI_TRANSPORT_ALL)\n",
    "\n",
    "let\n",
    "    carr = cu(rand(10_000_000))\n",
    "    pmap(WorkerPool(procs()), 1:nprocs()) do i\n",
    "        @btime CUDA.@sync sin.($carr) .+ 1\n",
    "    end\n",
    "end\n",
    "\n",
    "MPIClusterManagers.stop_main_loop(mgr)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd5c933",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Then `sbatch test_script.jl`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9775252-0ffd-47b9-9d3b-f161257bd22b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Here, movement of memory between GPUs will happen via CUDA MPI transport üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da0ab8d-683f-4af1-8df5-2950d74550af",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-GPU (multiple nodes, MPI, notebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1bb015-b678-49e3-82f8-ec8a86e42c07",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Some code in a notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e99065e-7209-4338-8f38-a1a389b5e238",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "let\n",
    "    carr = cu(rand(10_000_000))\n",
    "    pmap(WorkerPool(procs()), 1:nprocs()) do i\n",
    "        @btime CUDA.@sync sin.($carr) .+ 1\n",
    "        return nothing\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6e9c41-451f-4c4f-928b-79fc7c6b5023",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Now use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f29d5e1-5b56-4117-bcc6-76d90a962322",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "using ParameterizedNotebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c600c2-747b-4c6d-ab03-063fada5dde9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nb = ParameterizedNotebook(\"talk.ipynb\", sections=(\"Some code in a notebook:\",))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f025ce3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nb()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc4015d-af2c-4266-8cc0-ec9f3b384f44",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can put the call to the notebook code directly in a `test_script_2.jl`:\n",
    "```julia\n",
    "#!/bin/bash\n",
    "#SBATCH -C gpu -q regular -A mp107\n",
    "#SBATCH -t 00:05:00 \n",
    "#SBATCH --cpus-per-task 32 --gpus-per-task 1 --ntasks-per-node 4 --nodes 4\n",
    "#=\n",
    "srun /global/u1/m/marius/.julia/juliaup/julia-1.9.3+0.x64.linux.gnu/bin/julia $(scontrol show job $SLURM_JOBID | awk -F= '/Command=/{print $2}')\n",
    "exit 0\n",
    "# =#\n",
    "\n",
    "using MPIClusterManagers, Distributed, CUDA\n",
    "mgr = MPIClusterManagers.start_main_loop(MPIClusterManagers.MPI_TRANSPORT_ALL)\n",
    "\n",
    "nb = ParameterizedNotebook(\"talk.ipynb\", sections=(\"Some code in a notebook:\",))\n",
    "nb()\n",
    "\n",
    "MPIClusterManagers.stop_main_loop(mgr)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbe2c3b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "With some care in the organization of your sections, you can iterate on code in the notebook, even test it in parallel using on-the-fly `ElasticManager` workers, then submit the identical code as an MPI job for larger-scale runs üéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3405e4d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec83b8be-279c-47be-822f-d47f9986adfa",
   "metadata": {},
   "source": [
    "* Julia + Jupyter + GPUs offer powerful scientific workflows\n",
    "* Hopefully I've shared some efficient ways to do this that we've learned\n",
    "* Wishlist\n",
    "    * More robust and easier CUDA.jl task/threading support\n",
    "    * An easy way to use MPI CUDA transport protocol from within Jupyter jobs\n",
    "    * A _multi-node_ GPU monitor, even just a command-line one\n",
    "        * `nvitop`, `btop` (PR), and `gpustat` are some good command line single-node options"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
